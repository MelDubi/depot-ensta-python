{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEPJIQCyoNmx"
   },
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font>   |\n",
    "|:-------------:     |:-----------   |:------  |\n",
    "| monôme/binôme 1  | <span style=\"color:red\">Remplacer ici</span> | <span style=\"color:red\">et ici</span>     |\n",
    "| binôme 2         | <span style=\"color:red\">Remplacer ici</span> | <span style=\"color:red\">et ici</span>     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myeyciaHDG4K"
   },
   "source": [
    "# <center> **Reinforcement Learning by (deep) $Q$-learning** </center>\n",
    "## <center> Machine Learning Programming Exercise 13</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ce TP concerne la découverte de l'apprentissage par renforcement appliqué au domaine du gaming. Les observations seront données par un émulateur de jeux atari. L'objectif de ce tp est la mise en oeuvre de différents algorithmes de type **$Q$-learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DKpWekoDG4Q"
   },
   "source": [
    "# 1. Import useful packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_3jJXzjkwNZ"
   },
   "source": [
    "## 1.1 Colab or not colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6UnuzZuk8ZG",
    "outputId": "f4906567-c815-4616-965c-e0abd0be5688"
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "\n",
    "  # ipynb_name = 'tp_rl_qlearning_startercode_name1_name2.ipynb'\n",
    "\n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVLobehkk-zN"
   },
   "source": [
    "## 1.2 Import common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B7FhSgcuDG4R",
    "outputId": "a0f0918a-500b-472d-c917-94526a51b717"
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "\n",
    "# display imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "matplotlib.rc('animation', html='jshtml') # To get smooth animations\n",
    "\n",
    "\n",
    "\n",
    "# ML imports\n",
    "import tensorflow as tf\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "elif len(tf.config.list_physical_devices('GPU')) > 1:\n",
    "  # a décommenter si problème avec le GPU de votre machine\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "  for gpu in physical_devices:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "# RL imports:  install specific packages for openai/gym envs\n",
    "# install gym\n",
    "%pip install -q -U gymnasium\n",
    "%pip install -q -U gymnasium[classic_control]\n",
    "%pip install pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUgUEezHe9vC"
   },
   "source": [
    "## 1.3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5AU59QHe9_8"
   },
   "outputs": [],
   "source": [
    "# initialisation des graines aléatoires\n",
    "SEED = 33\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3ar_brzLXJG"
   },
   "source": [
    "## 1.4 some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJHFaL9uLg6v"
   },
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwf2U0jrDG4p"
   },
   "source": [
    "# 2.  Processus Markoviens de Décisions (MDP)\n",
    "\n",
    "Les MDP ont été décrits pour la première fois dans les années 50 par Richard Bellman à partir des chaînes de Markov (développés au début du 20ème siècle par Andrey Markov).\n",
    "\n",
    "\n",
    "## 2.1 Chaînes de Markov\n",
    "\n",
    "Les chaînes de Markov sont des processus stochastiques (aléatoires) possédant:\n",
    "- une nombre d'états fixes\n",
    "- la possibilité de basculer aléatoirement d'un état à un autre\n",
    "- la propritée que la probabilité de passage d'un état $s$ à un autre état $s'$ (probabilité de transitions) ne dépend que de l'état courant $s$ (processus sans mémoire, hypothèse de Markov)\n",
    "\n",
    "Il est facile d'en définir une et de la simuler. Soit la chaîne de Markov suivante:\n",
    "<table align=\"center\">\n",
    "<td align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1h6f0H2yOaFAAk53FTX2swG_AsBQ25hnh\" height=\"200px\" />\n",
    "<td/>\n",
    "</table>\n",
    "\n",
    "Le code suivant permet de la définir et de la simuler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_o1_kMhDG4p",
    "outputId": "44cdd42d-604d-40a7-a5fc-e9e169c8aa5a"
   },
   "outputs": [],
   "source": [
    "# init graine aléatoire\n",
    "SEED = 33\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Définition des proba de transition par une matrice de la taille**2 de l'espace d'action\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to ...\n",
    "\n",
    "N_ITERATIONS = 50\n",
    "\n",
    "#\n",
    "def generate_and_print_sequence():\n",
    "\n",
    "    # initialisation\n",
    "    current_state = 0 # start from that state\n",
    "    print(\"States sequence:\", end=\" \")\n",
    "\n",
    "    #\n",
    "    for step in range(N_ITERATIONS):\n",
    "        # display\n",
    "        print(current_state, end=\" \")\n",
    "\n",
    "        # pourquoi?\n",
    "        if current_state == 3:\n",
    "            break\n",
    "\n",
    "        # génération aléatoire de l'état suivant\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# call 10 times\n",
    "for _ in range(10):\n",
    "    generate_and_print_sequence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpdtIGNo9jkf"
   },
   "source": [
    "## 2.2 Processus Markoviens de Décisions (MDP)\n",
    "\n",
    "Les MDP ressemblent aux chaînes de Markov, mais avec une particularité : à chaque étape, un agent peut choisir une des actions possibles, et les probabilités de transition dépendent de l'action choisie. En outre, certaines transitions entre états donnent lieu à une récompense (positive ou négative), et l'objectif de l'agent est de trouver une `policy` qui maximisera les récompenses au fil du temps.\n",
    "\n",
    "<table align=\"center\">\n",
    "<td align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1nB13UH5AEjiOZ5b7hCSq3DUl_tmJGd7n\" height=\"300px\" />\n",
    "<td/>\n",
    "</table>\n",
    "\n",
    "Par exemple, le MDP représenté dans la figure ci-dessus comporte trois états et jusqu'à trois actions discrètes possibles à chaque étape. Si il commence à l'état $s_0$ , l'agent peut choisir entre les actions $a_0$ , $a_1$ ou $a_2$ . S'il choisit l'action $a_1$ , il reste simplement dans l'état $s_0$ avec certitude et sans aucune récompense. Il peut donc décider d'y rester pour toujours s'il le souhaite. Mais s'il choisit l'action $a_0$ , il a une probabilité de 70% d'obtenir une récompense de +10 et de rester dans l'état $s_0$.\n",
    "\n",
    "Il peut alors essayer encore et encore d'obtenir le plus de récompense possible. Mais à un moment donné, il finira plutôt dans l'état $s_1$ . Dans l'état $s_1$, il n'a que deux actions possibles : $a_0$ ou $a_1$ . Il peut choisir de rester sur place en choisissant à plusieurs reprises l'action $a_1$ , ou il peut choisir de passer à l'état $s_2$ et obtenir une récompense négative de -50. Dans l'état $s_3$, il n'a pas d'autre choix que de prendre l'action $a_1$, qui le ramènera très probablement à l'état $s_0$ , et obtiendra une récompense de +40 en cours de route.\n",
    "\n",
    "En examinant ce MDP, pouvez-vous deviner quelle stratégie sera la mieux récompensée au fil du temps ? Dans l'état $s_0$, il est clair que l'action $a_0$ est la meilleure option, et dans l'état $s_3$, l'agent n'a pas d'autre choix que de prendre l'action $a_1$ , mais dans l'état $s_1$, il n'est pas évident que l'agent doive rester sur place ($a_0$) ou passer à travers le feu ($a_2$).\n",
    "\n",
    "**Question 2.1 (réponse donnée): définissez le MDP via:**\n",
    "- **les probabilités de transition p(s'|s,a) comme une liste de listes de taille (taille(s),taille(a),taille(s')). Vous mettrez la valeur None si la probabilité n'est pas connue.**\n",
    "- **les récompenses comme une liste de listes de la même taille**\n",
    "- **les actions possibles dans chaque état: liste de listes de taille 3**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d3kE2wz93u4"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# transition_probabilities shape=[len(s), len(a), len(s')]\n",
    "transition_probabilities = [\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "\n",
    "# from state s_0 and taking action a_0, the probabilities to go to state s_0 is 0.7, s_1 is 0.3 and s_2 is 0.0\n",
    "# from state s_0 and taking action a_1, the probabilities to go to state s_0 is 1.0, s_1 is 0.0 and s_2 is 0.0\n",
    "# from state s_0 and taking action a_2, the probabilities to go to state s_0 is 0.8, s_1 is 0.2 and s_2 is 0.0\n",
    "# from state s_1 and taking action a_0, the probabilities to go to state s_0 is 0.0, s_1 is 1.0 and s_2 is 0.0\n",
    "# from state s_1 and taking action a_1, is not permitted\n",
    "# and so on\n",
    "\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "\n",
    "# from state s_0 and taking action a_0, the rewards associated to the transition to the state s_0 is 10, s_1 is 0 and s_2 is 0\n",
    "# and so on\n",
    "\n",
    "# the possible actions from state s_0, s1, s_3\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]\n",
    "\n",
    "# ------------------------------------------------>\n",
    "n_actions = len(possible_actions)\n",
    "n_states = len(transition_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN45dQvRDG4q"
   },
   "source": [
    "# 3. Q-value Iteration algorithm\n",
    "\n",
    "**Question 3.1: Ecrivez l'algorithme Q-value iteration (cf. slides) permettant de résoudre ce MDP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZxE26_Lwq6S"
   },
   "outputs": [],
   "source": [
    "# Set the discount factor\n",
    "GAMMA = 0.90\n",
    "\n",
    "# Set the max number of iterations\n",
    "N_ITERATIONS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mofv4MxYDG4q"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize all the Q-values\n",
    "q_values = np.full((n_states, n_actions), -np.inf) # -np.inf for impossible actions\n",
    "for state, action in enumerate(possible_actions):\n",
    "  q_values[state, action] = 0.0  # for all possible actions\n",
    "\n",
    "\n",
    "# boucle sur les iterations\n",
    "qiteration_history = [] # sera utilisé dans la suite du tp pour comparer à l'algo du Q-learning\n",
    "for step in range(N_ITERATIONS):\n",
    "\n",
    "  # copy to do a one-line update\n",
    "  q_values_current = q_values.copy()\n",
    "  qiteration_history.append(q_values_current) # update history\n",
    "\n",
    "  # boucle sur les états\n",
    "  for state in range(n_states):\n",
    "\n",
    "    # boucle sur les actions possibles de l'état s\n",
    "    for action in possible_actions[state]:\n",
    "        # q_value iteration update with the Bellman equation :\n",
    "        tmp = 0.\n",
    "        for pred_state in range(n_states):\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "            # tmp...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "        q_values[state, action] = tmp\n",
    "\n",
    "\n",
    "qiteration_history = np.array(qiteration_history) #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gabCFktB-1NQ",
    "outputId": "1b7bce84-360a-4474-98d9-9ff0c8ac6662"
   },
   "outputs": [],
   "source": [
    "#Vous devez obtenir:\n",
    "#array([[18.91891892, 17.02702702, 13.62162162],\n",
    "#       [ 0.        ,        -inf, -4.87971488],\n",
    "#       [       -inf, 50.13365013,        -inf]])\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmM7OKq1-x8Y"
   },
   "source": [
    "**Question 3.2: Décrivez et analysez la table des Q_valeurs. Compte tenu du résultat quelle est la policy/stratégie optimale? Analysez la.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3qirrhpySLE"
   },
   "source": [
    "**_`Your commented code below`_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXIrVql3-_bY",
    "outputId": "8e210a47-b8e6-401e-81f5-f5d3e0e1f530"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGC_cVOaySLF"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHQOpMTV_Lwh"
   },
   "source": [
    "**Question**: Réessayer avec un discount factor de .95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3YleIpwxFJw"
   },
   "outputs": [],
   "source": [
    "# Set the discount factor\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Set the max number of iterations\n",
    "N_ITERATIONS = 50 # nb d'iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nR44mgzB_Kpv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize all the Q-values\n",
    "q_values = np.full((n_states, n_actions), -np.inf) # -np.inf for impossible actions\n",
    "for state, action in enumerate(possible_actions):\n",
    "  q_values[state, action] = 0.0  # for all possible actions\n",
    "\n",
    "\n",
    "# boucle sur les iterations\n",
    "qiteration_history = [] # sera utilisé dans la suite du tp pour comparer à l'algo du Q-learning\n",
    "for step in range(N_ITERATIONS):\n",
    "\n",
    "  # copy to do a one-line update\n",
    "  q_values_current = q_values.copy()\n",
    "  qiteration_history.append(q_values_current) # update history\n",
    "\n",
    "  # boucle sur les états\n",
    "  for state in range(n_states):\n",
    "\n",
    "    # boucle sur les actions possibles de l'état s\n",
    "    for action in possible_actions[state]:\n",
    "        # q_value iteration update with the Bellman equation :\n",
    "        tmp = 0.\n",
    "        for pred_state in range(n_states):\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "#             tmp ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "        q_values[state, action] = tmp\n",
    "\n",
    "\n",
    "qiteration_history = np.array(qiteration_history) #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQipq8V9GMGi"
   },
   "source": [
    "**Question 3.3**: Donnez et décrivez la signification de $\\gamma$\n",
    " et analysez la nouvelle table de Q_valeurs. Compte tenu du résultat quelle est la policy/stratégie optimale? Analysez la réponse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be5YcCMjySLG"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njH1TSBq_W8l"
   },
   "source": [
    "# 4. Q-Learning algorithm\n",
    "\n",
    "Le problème du Q-value Iteration algorithm est qu'il faut connaitre parfaitement le modèle de l'environnement (les probabilités de transition).\n",
    "\n",
    "Nous allons exploiter l'algorithme du $Q$-Learning qui\n",
    "- suppose que l'agent ne connaît initialement que les états, les actions possibles et les rewards (**interdiction d'utiliser les probabilités de transition**)\n",
    "- utilise une **politique d'exploration** pour explorer le MDP (**Dans ce cas, on n'utilise pas uniquement la politique pour déterminer l'action**)\n",
    "- utilise la `difference temporelle` ($TD$ learning) pour estimer $Q^*$ satisfaisant à l'équation d'optimalité de Bellman\n",
    "- met progressivement à jour les estimations des valeurs $ Q $ en fonction des transitions entre états et des récompenses réellement observées.\n",
    "\n",
    "L'algorithme que nous allons programmer est dans les slides de cours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WLEn75aySLH"
   },
   "source": [
    "## 4.1 Selection de l'action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPr2RMM0MDrK"
   },
   "source": [
    "Nous avons également besoin d'une stratégie/`policy` d'exploration, qui peut être n'importe quel type de `policy`/stratégie, à condition qu'elle visite tous les états possibles à plusieurs reprises. Nous utiliserons ici une **`policy` purement aléatoire**, puisque l'espace des états est ici très restreint.\n",
    "\n",
    "**Question 4.1: Nous devons d'abord simuler un agent se déplaçant dans l'environnement, alors définissez une fonction `select_action` permettant t'effectuer une action aléatoire sachant qu'on est dans l'état `state` parmi les action possibles de cet état.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH7KAsxFMFiL"
   },
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "\n",
    "    # choix aléatoire uniforme d'une action possible  à réaliser dans l'état state\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # action = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnAG69pOy-MD"
   },
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_2h0hPkySLI",
    "outputId": "946c0022-67f6-4d26-9913-611bf2d5ba08"
   },
   "outputs": [],
   "source": [
    "SEED = 33\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print('(false) random action from state 0: {} (expected 0)'.format(select_action(0)))\n",
    "print('(false) random action from state 1: {} (expected 2)'.format(select_action(1)))\n",
    "print('(false) random action from state 2: {} (expected 0)'.format(select_action(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9LqVY7-ySLJ"
   },
   "source": [
    "## 4.2 Selection de l'état suivant et de la récompense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgCWWS_YH9O9"
   },
   "source": [
    "**Question 4.2: A partir de l'action `action` sélectionnée et de l'état courant `state`  obtenir le nouvel état aléatoirement et la récompense associée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0vVQzxNL0hi"
   },
   "outputs": [],
   "source": [
    "def select_nextstate_and_reward(state, action):\n",
    "\n",
    "    # choix aléatoire uniforme du prochain état parmi tous les états possibles\n",
    "\n",
    "    # ----------- Your code here --------------------->\n",
    "\n",
    "    # next_state = ...\n",
    "\n",
    "    # ------------------------------------------------>\n",
    "\n",
    "\n",
    "    # lecture de la récompense pour avoir réalisé l'action action dans l'état state\n",
    "    # et étant passé à l'état next-state\n",
    "\n",
    "    # ----------- Your code here --------------------->\n",
    "\n",
    "        # reward = ...\n",
    "\n",
    "    # ------------------------------------------------>\n",
    "\n",
    "    # sortie\n",
    "    return int(next_state), int(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLxgSo1ey8Tw"
   },
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC-LegwkySLK",
    "outputId": "2ff61917-ef0a-44fd-9908-91bf080bd8f8"
   },
   "outputs": [],
   "source": [
    "SEED = 33\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "        print('from state {} taking action {}, (next state, reward): {}'.format(s, a, select_nextstate_and_reward(s,a)))\n",
    "\n",
    "\n",
    "# expected values\n",
    "# from state 0 taking action 0, (next state, reward): (0, 10)\n",
    "# from state 0 taking action 1, (next state, reward): (1, 0)\n",
    "# from state 0 taking action 2, (next state, reward): (1, 0)\n",
    "# from state 1 taking action 0, (next state, reward): (0, 0)\n",
    "# from state 1 taking action 1, (next state, reward): (1, 0)\n",
    "# from state 1 taking action 2, (next state, reward): (1, 0)\n",
    "# from state 2 taking action 0, (next state, reward): (2, 0)\n",
    "# from state 2 taking action 1, (next state, reward): (1, 0)\n",
    "# from state 2 taking action 2, (next state, reward): (2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC8w0abPySLL"
   },
   "source": [
    "## 4.3 Selection de l'état suivant et de la récompense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NecPClItMLQ1"
   },
   "source": [
    "**Question 4.3:** Ecrivez l'algorithme du $Q$-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RiB4kWGzCuq"
   },
   "outputs": [],
   "source": [
    "# Parametres\n",
    "N_ITERATIONS = 10000  # nb d'iterations\n",
    "GAMMA = 0.90 # discount factor\n",
    "LR_INIT = 0.05 # initial learning rate\n",
    "LR_DECAY = 0.005 # learning rate decay\n",
    "\n",
    "# initialisation de la graine\n",
    "SEED = 33\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUEIvtmpMNoE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialisation de la matrice des Q-valeurs à zero\n",
    "q_values = np.full((n_states, n_actions), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    q_values[state][actions] = 0.\n",
    "\n",
    "# Boucle d'interactions\n",
    "state = 0 # initial state\n",
    "qlearning_history = [] # sera utilisé pour comparer à Q_value iteration\n",
    "for step in range(N_ITERATIONS):\n",
    "\n",
    "    # update history\n",
    "    qlearning_history.append(q_values.copy())\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # select random action\n",
    "    # action = ...\n",
    "\n",
    "    # select next state and reward\n",
    "    # next_state = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # le learning rate décroit avec le nombre d'iteration\n",
    "    learning_rate = LR_INIT / (1 + step * LR_DECAY)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # calcul de la TD target pour Q\n",
    "    # td_target = ...\n",
    "\n",
    "    # mise à jour la matrice de Q_values\n",
    "    # q_values[] = ...\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # mise à jour de l'état courant\n",
    "    state = next_state\n",
    "\n",
    "qlearning_history = np.array(qlearning_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkBi_2dIMaAM",
    "outputId": "0b6083b4-7f26-4451-efe3-0bf1b0d5b28d"
   },
   "outputs": [],
   "source": [
    "# Test: array([[26.1907234 , 23.29105121, 23.3312448 ],\n",
    "#        [24.42046691,        -inf,  7.64111659],\n",
    "#        [       -inf, 39.38499652,        -inf]])\n",
    "q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmerYQhYNsok"
   },
   "source": [
    "**Question 4.4:** Quelle est la policy optimale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP0TMwbqNtKr",
    "outputId": "d0478367-9a37-4fcf-8592-3714c7b5c6cd"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fivNwtwN05a"
   },
   "source": [
    "\n",
    "**Grâce au code fourni, vous pouvez visualiser la vitesse de convergence des deux algorithmes et voir l'influence de ne pas connaître les probabilités de transition pour le Q-learning.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "mD8ZprzdN1UM",
    "outputId": "92ba3753-9f66-411e-ffed-5d0c267fd3f4"
   },
   "outputs": [],
   "source": [
    "true_Q_value = qiteration_history[-1, 0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].set_ylabel(\"Q-Value$(s_0, a_0)$\", fontsize=14)\n",
    "axes[0].set_title(\"Q-Value Iteration\", fontsize=14)\n",
    "axes[1].set_title(\"Q-Learning\", fontsize=14)\n",
    "\n",
    "n = qiteration_history.shape[0]\n",
    "axes[0].plot([0, n], [true_Q_value, true_Q_value], \"k--\")\n",
    "axes[0].plot(np.arange(n), qiteration_history[:, 0, 0], \"b-\", linewidth=2)\n",
    "axes[0].axis([0, n, 0, 100])\n",
    "\n",
    "n = qlearning_history.shape[0]\n",
    "axes[1].plot([0, n], [true_Q_value, true_Q_value], \"k--\")\n",
    "axes[1].plot(np.arange(n), qlearning_history[:, 0, 0], \"b-\", linewidth=2)\n",
    "axes[1].axis([0, n, 0, 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPI847LSnh_U"
   },
   "source": [
    "# 5. Introduction to OpenAI gym (facultatif si vous l'avez déjà fait)\n",
    "\n",
    "C'est le même que dans le tp sur les algorithmes `policy gradient`. Si vous l'avez déjà fait, passez directement à la section suivante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l59SaYwgzRm"
   },
   "source": [
    "Dans ce notebook, l'environnement et les observations/données seront fournis par le framework [OpenAI gym](https://gym.openai.com/). Il fournit de nombreux environnements avec lesquels peut interagir et apprendre votre *agents*.\n",
    "L'import se fait classiquement par:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "GrCXxNlwnrSV",
    "outputId": "96d0d943-aaad-4b93-e1bb-a4bd4e4c29b5"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPNgyIn6nz-s"
   },
   "source": [
    "La liste des environnements disponibles est donnée par:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzcHYvi_n0n3",
    "outputId": "74684768-4480-4402-96a2-6f2de96aba32"
   },
   "outputs": [],
   "source": [
    "g = gym.envs.registry\n",
    "for l in list(g):\n",
    "  print(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmKFtOFUn6oC"
   },
   "source": [
    "## 5.1 Le Cart-Pole\n",
    "\n",
    "Le Cart-Pole est un environnement très simple dans lequel un chariot peut bouger soit vers la gauche, soit vers la droite. Un bâton/piquet est placé sur ce dernier. L'agent doit alors bouger le chariot à gauche ou à droite pour que le bâton reste droit.\n",
    "<table align=\"center\">\n",
    "<td align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1RNWq3McGaTydH2PrNQWFcXLxV7Lg2mC8\" height=\"300px\" />\n",
    "<td/>\n",
    "</table>\n",
    "\n",
    "\n",
    "L'environnement est défini par la commande make():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_y6Q4JXn8DY"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KKl6BagoAly"
   },
   "source": [
    "Il faut ensuite initialiser l'environnement en appelant la méthode `reset()` qui retourne la première observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9PNXG-ToFFf",
    "outputId": "9a8b1c26-4356-41de-f531-97ea5a74e973"
   },
   "outputs": [],
   "source": [
    "SEED = 33\n",
    "obs = env.reset(seed=SEED)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkqFKyjWoTq9"
   },
   "source": [
    "Les observations dépendent de l'environnement considéré. Dans ce cas, les observations sont regroupées dans un array Numpy 1D soit 1 vecteur composé de 4 floats. Ce vecteur regroupe:\n",
    "- la position horizontale du chariot (<0 gauche; 0 = verticale; >0 droite)\n",
    "- la vitesse du chariot  \n",
    "- l'angle du bâton (<0 gauche; 0 = verticale; >0 droite)\n",
    "- la vitesse angulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsOFNzBroYH9"
   },
   "source": [
    "Un environnement peut-être visualisé par appel à la méthode `render()`. Pour récupérer une observation (une image) de l'environnement sous la forme d'un array NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUYHwEjEqvvI",
    "outputId": "3dcc1662-2ed2-454a-cf30-d4edfb025a8c"
   },
   "outputs": [],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxX08Df6rAmp"
   },
   "source": [
    "La fonction plot_environment() permet de récupérer et d'afficher l'observation courante de l'environnement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEX4moMsrDDk"
   },
   "outputs": [],
   "source": [
    "def plot_environment(env, figsize=(5,4)):\n",
    "    # get an observation\n",
    "    img = env.render()\n",
    "\n",
    "    # display an observation\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwYIuDNMrUWd"
   },
   "source": [
    "Pour interagir avec l'environnement, l'agent doit sélectionner une action à partir de l'espace des actions possibles. Pour chaque environnement, la commande `action_space` permet de connaitre cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogG2iwk9rU1d",
    "outputId": "4fe08ee1-29ec-47fb-f070-313937d8c439"
   },
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUn_Cw03rXUF"
   },
   "source": [
    "Pour le cartpole,deux actions sont possible: gauche ou droite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBCDZuU6rb8T"
   },
   "source": [
    "Dans l'état actuel, le bâton est penché vers la droite (`obs[2] > 0`); passons l'action de déplacer le chariot vers la droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sdp7bFeurY6W",
    "outputId": "89721abf-8647-4aa4-9792-c2dc8bb3a2e2"
   },
   "outputs": [],
   "source": [
    "# action\n",
    "go_right = True  # or 1, go right\n",
    "go_right = False # or 0, go right\n",
    "\n",
    "# interaction with the environment\n",
    "obs, reward, done, truncated, info = env.step(go_right)\n",
    "\n",
    "# observation\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbvuNM5vreyZ"
   },
   "source": [
    "Remarquez que:\n",
    "- le chariot se déplace maintenant vers la droite (`obs[1] > 0`);\n",
    "- le bâton est toujours incliné vers la droite (`obs[2] > 0`) ;\n",
    "- mais sa vitesse angulaire est maintenant négative (`obs[3] < 0`), il est ainsi vraissemblable qu'il soit incliné vers la gauche après la prochaine action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U4D1l1R0rkvk",
    "outputId": "b3468ee1-2766-4993-f9e4-25c7009d937e"
   },
   "outputs": [],
   "source": [
    "plot_environment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iPVJKUJrwKa"
   },
   "source": [
    "L'interaction avec l'environnement:\n",
    "- génère l'observation suivante `obs`\n",
    "- renseigne l'agent sur la récompense générée par l'action prise: `reward`\n",
    "- retourne `done=True` si le jeu est fini (pour cet environnement, le jeu est fini si l'angle du baton est de plus de 12° ou si le chariot s'est déplacé de plus 2.4 unités par rapport au centre.)\n",
    "- `truncated`: cette valeur sera True lorsqu'un épisode est interrompu prématurément, par exemple par un wrapper d'environnement qui impose un nombre maximum d'étapes par épisode (voir la documentation de Gym pour plus de détails sur les wrappers d'environnement).\n",
    "- info est un dictionnaire spécifique à chaque environnement qui peut contenir des informations supplémentaires pour le debugging ou l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhIURDHkrx8a",
    "outputId": "555deb70-94ee-408a-89c8-7bddf9cdefc3"
   },
   "outputs": [],
   "source": [
    "print(reward, done, truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLIJ5TsZr3_n"
   },
   "source": [
    "Comme nous l'avons vu en cours, un **épisode** correspond à la séquence d'interactions entre le moment où l'environnement est lancé `reset()` et le moment ou `done=True`. Pour relancer un épisode, il faut refaire appel à la méthode `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7x9YQ-nr_Bq"
   },
   "outputs": [],
   "source": [
    "if done or truncated:\n",
    "    obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieLpK0GFh1RS"
   },
   "source": [
    "# 6. Définition d'une politique simple (policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTwRBgdFr7F1"
   },
   "source": [
    "Revenons au **CartPole** et essayons de maintenir le bâton droit. Nous avons besoin d'une policy (politique/stratégie) pour cela. Il s'agit d'une stratégie que l'agent utilisera pour sélectionner une action à chaque étape. Cette stratégie pourra potentiellement utiliser toutes les actions et observations passées pour faire cela.\n",
    "\n",
    "Pour le moment, nous allons définir une politique très simple consistant à déplacer le chariot vers la gauche (resp. la droite) quand il penche vers la gauche (resp. la droite). Ici, la policy $\\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{o}_t)$ est déterministe pourra donc s'écrire $\\mathbf{a}_t = \\pi(\\mathbf{o}_t)$.\n",
    "\n",
    "**Question 6.1: Pour l'implémentation vous créerez:**\n",
    "- **une fonction `select_action_simple_policy(obs)` retournant l'action à prendre en fonction de l'observation**\n",
    "- **un script qui permet de**\n",
    " - **générer `N_EPISODES` (500) **épisodes** (ou **trajectoires**, **roll_out**)**\n",
    " - **chacun de ces épisodes comprend un nombre max d'interactions (de pas temporels) avec l'environnement fixé à `N_STEPS_MAX_PER_EPISODE = 200`**\n",
    " - **pour chaque interaction (pas temporel), décider de l'action à prendre (appel à `select_action_simple_policy()`), jouer cette action en interagissant avec l'environnement et récupérer la récompense et calculer la récompense totale**\n",
    " - **calculer les performances de l'algorithme (les statistiques: moyenne, std, min, max)**\n",
    "\n",
    "\n",
    "**Notes**: deux boucles imbriquées sont à coder, vous pouvez suivre le squellette suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh9hOqdwi6wN"
   },
   "outputs": [],
   "source": [
    "# policy definition\n",
    "def select_action_simple_policy(obs):\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # go_right = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "    return go_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnaxFto20hTZ"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# set random seed\n",
    "SEED = 33\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "obs = env.reset(seed=SEED)\n",
    "\n",
    "\n",
    "\n",
    "# parameters\n",
    "N_EPISODES = 500\n",
    "N_STEPS_MAX_PER_EPISODE = 200\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGas1Df9wyG5",
    "outputId": "348ff1ae-2793-41eb-c5f3-4236e925b3ce"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# boucle sur les épisodes/loop over the episodes\n",
    "episode_cumrewards_history=[]\n",
    "for episode in range(N_EPISODES):\n",
    "\n",
    "    # starting a new episode\n",
    "    episode_cumrewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "\n",
    "    # ...with N_STEPS_MAX_PER_EPISODE interactions (temps)\n",
    "    for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "      # starting a new interaction\n",
    "\n",
    "      # select an action\n",
    "      # go_right = ...\n",
    "\n",
    "      # interaction with the environment\n",
    "      # obs...\n",
    "\n",
    "      # compute cumulative reward\n",
    "      episode_cumrewards += reward\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "      # continue or not\n",
    "      if done or truncated:\n",
    "        print('break at {}: done={},  truncated={}'.format(step, done, truncated))\n",
    "        break\n",
    "\n",
    "    # performance statistics\n",
    "    episode_cumrewards_history.append(episode_cumrewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX_QCwOBjAi6"
   },
   "source": [
    "**Question 6.2: Affichez:**\n",
    "- **la récompense cumulée moyenne +/- écart type calculée sur les épisodes**\n",
    "- **l'évolution de la récompense cumulée au cours des épisodes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnjXm1pjlCyE"
   },
   "source": [
    "**_`Your commented code below`_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "3b9KnFdWx5fV",
    "outputId": "008366c6-c9f0-4838-a879-a9d8cc0fdc85"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "# expected value 41.698 +/- 8.389445512070509\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cT-jejCCylFP"
   },
   "source": [
    "**Questions 6.3**: Quelles sont vos conclusions sur les performances obtenues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ_FwziWym5Q"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    "\n",
    "**Solution**\n",
    "Comme on pouvait s'y attendre, cette stratégie est un peu trop basique : le mieux qu'elle ait fait est de maintenir l'interrogation pendant seulement 66 étapes. Cet environnement est considéré comme résolu lorsque l'agent maintient le sondage pendant 200 étapes.\n",
    "</FONT>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "japOnaRlyzzp"
   },
   "source": [
    "Vous pouvez visualiser l'évolution du cartpole au cours d'un episode avec la boucle suivante.\n",
    "\n",
    "**Question 6.4: Complétez les lignes demandées.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DIrwCaiPy3m4",
    "outputId": "8760febd-9541-494b-faf4-62df7616fea3"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# set random seed\n",
    "SEED = 33\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# boucle temporelle d'interactions, un step = un pas temporel = une interaction\n",
    "frames = []\n",
    "for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "    # recuperation de l'environnement courant\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "    # selectionner une action\n",
    "    # go_right=...\n",
    "# ------------------------------------------------>\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "    # interaction avec l'environnement\n",
    "    # obs...\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # continue or not\n",
    "    if done or truncated:\n",
    "        print('break at {}: done={},  truncated={}'.format(step, done, truncated))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dy8YdIJcr7JM"
   },
   "source": [
    "Vous pouvez définir un lecteur d'animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "_yyBVSQgzwrq",
    "outputId": "8b9f0a76-b7ed-424c-c553-6d772769c422"
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdYkkkOuAcBJ"
   },
   "source": [
    "# 8. Deep Q-Network et Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpO9vVP6Bo6d"
   },
   "source": [
    "Le principal problème du $Q$-Learning est qu'il n'est pas bien adapté aux MDP ayant un grand (ou même moyen) nombre d'états et actions (sans parler des cas où les états et les actions sont continues) car la matrice des Q-valeurs devient trop grande.\n",
    "\n",
    "La solution consiste à trouver une fonction qui approxime les $Q$-valeurs en utilisant une nombre de paramètres peu important. Pendant des années, il était recommandé d'utiliser des combinaisons linéaires de features extraits des états ou observations (par exemple, la distance des fantômes les plus proches, leurs directions, etc. dans le jeu Pacman) pour estimer les $Q$-valeurs, mais DeepMind a montré que l'utilisation de réseaux de neurones profonds fonctionne beaucoup mieux, en particulier pour les problèmes complexes, et cela ne nécessite aucune extraction *manuelle* de features. Un réseau de neurones utilisé pour estimer les $Q$-valeurs est appelé **deep $Q$-Network** (DQN), et l'utilisation d'un DQN pour l'approximation des $Q$-valeurs est appelée **Deep Q-Learning**.\n",
    "\n",
    "Ici nous allons appliquer cet algorithme au cas du cartpole (même si ici le nombre d'états et d'actions sont restreints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsyZMvgxBvrG"
   },
   "source": [
    "## 8.1 Création d'un réseau de neurones pour la q_values\n",
    "\n",
    "Pour résoudre l’environnement CartPole, nous n’avons pas besoin d’un réseau très complexe ; deux couches cachées suffisent.\n",
    "\n",
    "**Question 8.1: Définissez un réseau de neurones:**\n",
    "- **à 2 couches cachées dense de 32 neurones utilisant des fonctions d'activation `elu`**\n",
    "- **avec en entrée les états de l'environnement**\n",
    "- **et en sortie les actions possibles dans l'environnement.**\n",
    "\n",
    "**Que représentent les sorties de ce réseau?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejNVCUzVByFv",
    "outputId": "b08c308d-b180-453e-c9fe-55b26a5c4999"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set random seed\n",
    "SEED = 33\n",
    "\n",
    "# initialisation des graines\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "obs, info = env.reset(seed=SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# qvalues_model=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "# test du model\n",
    "q = qvalues_model(obs[np.newaxis])\n",
    "print(q)\n",
    "\n",
    "# tf.Tensor([[-0.00106677  0.00520632]], shape=(1, 2), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhFcTL0d0Vqs"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efeRbnpgB7NJ"
   },
   "source": [
    "## 8.2 Sélectionner une action selon une stratégie exploitation/exploration\n",
    "\n",
    "\n",
    "Nous allons sélectionner une action à l'aide de ce DQN selon une stratégie d'exploration appelée $\\epsilon$**-greedy policy** (elle est décrite dans mes slides). Pour la mettre en oeuvre, répondez aux questions suivantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1_UhHxMySLm"
   },
   "source": [
    "**Question 8.2: comment choisir l'action à partir `qvalues_model`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aQAuNip8mCK"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vm8QdX99U0P"
   },
   "source": [
    "**Question 8.3: comment fonctionne l'algorithme $\\mathbf{\\epsilon}$-greedy policy?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omzAIEgU8szu"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zbvhdth9uBN"
   },
   "source": [
    "**Mettez-la en oeuvre ici:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8_nDL9QB8Jz"
   },
   "outputs": [],
   "source": [
    "def select_action_epsilon_greedy_policy(state, epsilon=0):\n",
    "\n",
    "    if np.random.rand() < epsilon: # epsilon #tf rand?\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "        # action = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    else: # greedy\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "        # q_values = ...\n",
    "        # action = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    action  = int(action)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtI_jxcZCEhO"
   },
   "source": [
    "## 8.3 Experience replay buffer\n",
    "\n",
    "Deep Q-learning est un algorithme `off-policy` (cf slides de cours pour la différence entre `on/off policy`).\n",
    "\n",
    "\n",
    "Au lieu d’entraîner le DQN en fonction des dernières expériences uniquement, nous stockons toutes les expériences dans une mémoire (experience replay buffer ) et nous en extrayons un batch aléatoire à chaque itération d’entraînement. Cela permet de réduire les corrélations entre les expériences d’un batch d'entraînement et facilite énormément cette étape d'apprentissage.\n",
    "\n",
    "Ces expériences passées de l'agent (Experience replay) seront stockée sous forme de tuples : `(obs, action, récompense, next_obs, done)`. Nous pouvons utiliser la classe `deque` pour effectuer cela. Un deque est une liste chaînée dans laquelle chaque élément pointe sur le suivant et sur le précédent.  L’insertion et la suppression des éléments sont très rapides, mais plus la liste deque est longue, plus l’accès aléatoire est lent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeR7aRs6CGWS"
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIll4rxECKx4"
   },
   "source": [
    "Chaque expérience est constituée de cinq éléments : un état, l’action effectuée par l’agent, la récompense résultante, l’état suivant atteint et une valeur booléenne indiquant si l’épisode est à ce stade terminé (done). Nous avons besoin d’une petite fonction d’échantillonnage d’un lot aléatoire d’expériences à partir du replay buffer.\n",
    "\n",
    "Et créons une fonction pour sélectionner aléatoirement les expériences de la mémoire pour former un mini-batch  de données. Elle renverra 5 tableaux NumPy de taille batch_size: `[obs, actions, récompenses, next_obs, dones]`.\n",
    "A noter que des variantes existent pour sélectionner stratégiquement des expériences significatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vs1MDKXiCMlG"
   },
   "outputs": [],
   "source": [
    "def select_batch_from_replay_buffer(batch_size):\n",
    "\n",
    "    # selection aléatoire dans la mémoire\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "\n",
    "    # mise en liste\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "\n",
    "    # mise en numpy arrays du batch\n",
    "    states, actions, rewards, next_states, dones, truncateds = [np.array([experience[field_index] for experience in batch]) for field_index in range(6)]\n",
    "\n",
    "    # sortie\n",
    "    return states, actions, rewards, next_states, dones, truncateds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfyNmtI_-aia"
   },
   "source": [
    "## 8.4 Générer les données pour une interaction (un pas temporel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAAxIVUnCNMZ"
   },
   "source": [
    "Nous pouvons maintenant créer une fonction qu'utilisera le DQN pour :\n",
    "- selectionner une action selon une stratégie d'exploration/exploitation\n",
    "- interagir avec l'environnement (un pas temporel)\n",
    "- et enregistrer son expérience dans la mémoire buffer.\n",
    "\n",
    "**Question 8.4: Compléter la fonction `run_one_step`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rk-KjpN1COuE"
   },
   "outputs": [],
   "source": [
    "def run_one_step(env, state, epsilon):\n",
    "\n",
    "\n",
    "    # selection d'une action à partir de l'état\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # action = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # interaction avec l'environment\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # next_state...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "    # update replay buffer with state, action, reward, next_state, done, truncated\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "\n",
    "    # output\n",
    "    return next_state, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvR8LQ7mL3U-"
   },
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Ojg4nPJL5Fw",
    "outputId": "0df620da-9a12-4d93-f808-d8eb56b93b98"
   },
   "outputs": [],
   "source": [
    "# init\n",
    "# import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# set random seed\n",
    "SEED = 33\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# test\n",
    "state = 0\n",
    "epsilon = .5\n",
    "next_state, reward, done, truncated, info = run_one_step(env, state, epsilon)\n",
    "print(next_state, reward, done, truncated, info)\n",
    "# [-0.00549879 -0.18883361  0.04031888  0.28069958] 1.0 False False {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogrGM3h_-jed"
   },
   "source": [
    "## 8.5 Apprentissage de l'algorithme deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iMipgFACQkW"
   },
   "source": [
    "Enfin, nous allons créer une fonction qui va effectuer une étape d'entraînement par descente de gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmNEEtReB_fw"
   },
   "source": [
    "### 8.5.1 Paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIS28UcJB8eb"
   },
   "outputs": [],
   "source": [
    "# initialisation des paramètres et de l'optimiseur et de la fonction de coût\n",
    "N_EPISODES              = 600 #\n",
    "N_STEPS_MAX_PER_EPISODE = 200\n",
    "BATCH_SIZE              = 32\n",
    "DISCOUNT_FACTOR         = 0.95\n",
    "LEARNING_RATE           = 1e-2\n",
    "\n",
    "SEED                    = 33\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90wYZS0wwsXJ"
   },
   "source": [
    "### 8.5.2. Définir le modèle DQN approximant les Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd8y7ScIxRT4"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# Definition of the Q-values neural network\n",
    "# ----------------------------------->\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# qvalues_model = ...\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IJm9lhDxeiI"
   },
   "source": [
    "### 8.5.2  Choisir la tâche à apprendre et comment la résoudre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYzSFyrDxyir"
   },
   "outputs": [],
   "source": [
    "# Choix de la fonction de cout\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# loss_fcn=...\n",
    "\n",
    "# -------- END YOUR CODE HERE --------->\n",
    "\n",
    "# Choix de l'optimiseur\n",
    "optimizer_fcn = tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUYeH2N0CHZe"
   },
   "source": [
    "### 8.5.3 Apprentissage du DQN sur un batch d'expériences\n",
    "\n",
    "**Question 8.5: compléter le code ci-dessous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8ry8njYCSOl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# fonction réalisant un pas d'apprentissage\n",
    "def train_dqn_on_batch(batch_size):\n",
    "\n",
    "\n",
    "    # former le batch d'expériences (experience replay)\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # states...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Predire les q-valeurs les prochains états\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # next_q_values...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "    # calcul du max sur les actions des q-valeurs des prochains états\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # max_next_q_values...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculer la q_target ou TD target pour les q-valeurs\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # target_q_values=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # mise en forme\n",
    "    target_q_values = tf.reshape(target_q_values, (-1, 1)) # (batch_size,)\n",
    "    # print('target_q_values',target_q_values)\n",
    "\n",
    "    # encodage des actions en one-hot encoding (format pris par la loss function)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    # print('mask',mask)\n",
    "\n",
    "    # Calcul de la fonction de cout dans le contexte GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # forward pass of the model to estimate the Q_values\n",
    "        q_values = qvalues_model(states)\n",
    "\n",
    "        # calcul de la loss (à noter qu'on sélectionne uniquement les valeurs non nulles grâce au mask)\n",
    "        q_values = tf.reduce_sum(q_values * mask, axis=1, keepdims=True)\n",
    "\n",
    "        # calcul de la loss\n",
    "        loss = tf.reduce_mean(loss_fcn(target_q_values, q_values))\n",
    "\n",
    "\n",
    "    # Estimation des gradients de la fonction de cout\n",
    "    grads = tape.gradient(loss, qvalues_model.trainable_variables)\n",
    "\n",
    "    # Update par descente de gradient\n",
    "    optimizer_fcn.apply_gradients(zip(grads, qvalues_model.trainable_variables))\n",
    "\n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQoBbwZVskAX"
   },
   "source": [
    "### 8.5.4. Script final d'apprentissage DQN\n",
    "\n",
    "Finalement, on a tous les blocs pour l'algorithme final que vous compléterez (prenez les bouts de code plus haut si besoin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCC4ff2ix8jc"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# Création de l'environnement de simulation\n",
    "# ----------------------------------->\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# initialisation des graines aléatoires\n",
    "obs = env.reset(seed=SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "Y5y5W18lCYlo",
    "outputId": "a1e96dd1-add4-4ede-a267-bbecb3d74726"
   },
   "outputs": [],
   "source": [
    "\n",
    "episodes_reward = []\n",
    "episodes_loss = []\n",
    "\n",
    "best_score = 0\n",
    "for episode in range(N_EPISODES):\n",
    "\n",
    "    # init de l'environnement\n",
    "    step_obs, step_info = env.reset()\n",
    "\n",
    "    # génération de données en interagissant avec l'environnement\n",
    "    # le buffer se remplit...\n",
    "    for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "        # stratégie d'exploration: les valeurs d'epsilon s'étalent linéairement de 1 à 0.001\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "\n",
    "        # Générer les données pour une interaction (un pas temporel)\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "        # step_obs, step_reward, step_done, step_info...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "        if step_done or step_truncated:\n",
    "            break\n",
    "\n",
    "\n",
    "    # update rewards obtained\n",
    "    episodes_reward.append(step) #\n",
    "    print(\"\\rEpisode: {}, Rewards: {}, Best Rewards: {}, eps: {:.3f}\".format(episode, step + 1, best_score + 1, epsilon), end=\"\")\n",
    "\n",
    "\n",
    "    # performance: pour cart-pole plus step est grand plus le score est bon\n",
    "    if step > best_score: #\n",
    "        best_weights = qvalues_model.get_weights() #\n",
    "        best_score = step #\n",
    "\n",
    "\n",
    "    # une fois le buffer suffisamment rempli, on peut commencer à sélectionner et entrainer sur des batchs\n",
    "    if episode > 50:\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "        # loss...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "        \n",
    "      episodes_loss.append(loss) #\n",
    "\n",
    "    else:\n",
    "      episodes_loss.append([])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Eventually, update qvalues_model with best_weights\n",
    "qvalues_model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srgxpmlxXm6c"
   },
   "source": [
    "**Question: examinez et commentez les récompenses obtenues tout au long de l'apprentissage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "nnJXFFfUCc1d",
    "outputId": "11cbad28-cda1-4298-81e3-19a64681db06"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HA25wouhqeU"
   },
   "source": [
    "Double-cliquez pour donner votre réponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq-XSocjm4UK"
   },
   "source": [
    "### 8.5.5. Test de la Q-value apprise\n",
    "\n",
    "**Question 6.5: complétez enfin ce code permettant de tester la Q-value apprise**\n",
    "\n",
    "\n",
    "\n",
    "**Attention!! pas d'exploration en mode test!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCqYEpB9pAtX"
   },
   "outputs": [],
   "source": [
    "# parametres\n",
    "N_STEPS_MAX_PER_EPISODE = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlCU14GTChm6"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# test de la Q-value apprise\n",
    "# ----------------------------------->\n",
    "\n",
    "# initialisation de l'environnement\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# on fixe la graine aléatoire\n",
    "# env.seed(SEED)\n",
    "\n",
    "# recupération de la première obs/state\n",
    "state, info = env.reset(seed=SEED)\n",
    "\n",
    "\n",
    "# boucles de N_STEPS_MAX_PER_EPISODE interactions max\n",
    "frames = []\n",
    "steps_rewards = []\n",
    "for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "  # recuperation de l'environnement courant\n",
    "  frames.append(env.render())\n",
    "\n",
    "  # selection d'une action à partir de l'état\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "  # action=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "  # interaction avec l'environnement\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "  # state,...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # mise à jour de la liste des rewards\n",
    "  steps_rewards.append(reward)\n",
    "\n",
    "  # break or not\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# plt.plot(np.arange(len(rewards)), np.array(steps_rewards))\n",
    "plt.plot(np.array(steps_rewards))\n",
    "plt.xlabel(\"step []\", fontsize=14)\n",
    "plt.ylabel(\"Reward []\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrPAOvjdySLx"
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
