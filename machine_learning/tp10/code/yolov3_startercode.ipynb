{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Object detection** </center> \n",
    "## <center> Machine Learning Programming Exercise 10</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font> |\n",
    "|:-------------: |:----------- |:------ |\n",
    "| monôme/binôme 1 | <span style=\"color:red\">Remplacer ici</span> | <span style=\"color:red\">et ici</span> |\n",
    "| binôme 2 | <span style=\"color:red\">Remplacer ici</span> | <span style=\"color:red\">et ici</span> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail, vous apprendrez à détecter des objets en utilisant le modèle YOLO version 3. \n",
    "\n",
    "**Vous apprendrez à** :\n",
    "- Utiliser la détection d'objets sur un jeu de données de détection de voitures ou sur les images de votre choix.\n",
    "- Traiter les boîtes englobantes et appliquer la procédure d'inférence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Colab or not colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True) \n",
    "\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "\n",
    "  ipynb_name = '.ipynb'\n",
    "  \n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls\n",
    "   \n",
    "  # import special packages for colab\n",
    "  from google.colab.patches import cv2_imshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "    \n",
    "# common packages\n",
    "import numpy as np\n",
    "\n",
    "# plot packages\n",
    "# import matplotlib\n",
    "# matplotlib.use(\"TkAgg\") # permet d'avoir le mode ion sur pycharm\n",
    "\n",
    "# image proc packages\n",
    "import cv2\n",
    "import colorsys\n",
    "\n",
    "\n",
    "# ml packages\n",
    "import tensorflow as tf\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "elif len(tf.config.list_physical_devices('GPU')) > 1:\n",
    "  # a décommenter si problème avec le GPU de votre machine\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "  for gpu in physical_devices:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Position du problème \n",
    "\n",
    "\n",
    "Vous travaillez sur une voiture à conduite autonome. En tant que composante essentielle de ce projet, vous souhaitez d'abord construire un système de détection des voitures. Pour recueillir des données, vous avez monté une caméra sur le capot (c'est-à-dire à l'avant) de la voiture, qui prend des photos de la route toutes les quelques secondes pendant que vous roulez. \n",
    "\n",
    "\n",
    "    \n",
    "<center>\n",
    "<video width=\"400\" height=\"200\" src=\"https://drive.google.com/uc?export=view&id=1dvZYjgbkWpOzGMYOtninh22wuaSvU9_T\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "    \n",
    "</center>\n",
    "\n",
    "Vous avez rassemblé toutes ces images dans un dossier et les avez étiquetées en dessinant des boîtes de délimitation autour de chaque voiture que vous avez trouvée. Voici un exemple de ce à quoi ressemblent vos boîtes englobantes.\n",
    "\n",
    "Si vous avez 80 classes que vous voulez que YOLO reconnaisse, vous pouvez représenter l'étiquette de classe $c$ \n",
    "- soit sous la forme d'un entier de 1 à 80, \n",
    "- soit sous la forme `one-hot` d'un vecteur à 80 dimensions (avec 80 nombres) dont une composante est 1 et les autres sont 0. \n",
    "\n",
    "\n",
    "Dans cet exercice, vous apprendrez comment fonctionne YOLO, puis vous l'appliquerez à la détection de voitures. L'entraînement du modèle YOLO étant très coûteux en termes de calcul, nous chargerons des poids pré-entraînés que vous pourrez utiliser. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Description de YOLOv3\n",
    "\n",
    "YOLOv3 (\"you only look once\") est un algorithme très -populaire parce qu'il atteint une grande précision tout en étant capable de fonctionner en temps réel. Cet algorithme \"ne regarde qu'une fois\" l'image en ce sens qu'il n'a besoin que d'un seul passage de propagation vers l'avant à travers le réseau pour faire des prédictions. Après une étape de post-processing appelée non-maximal-suppression, il fait des prédictions de présence d'objets dans l'image associée à un boîtes englobante ( appelée par la suite **bbox** pour bounding box aussi appelé anchor box).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Boites englobantes\n",
    "Chaque boîte englobante bbox (Figure 1) est représentée par 6 grandeurs $(b_x, b_y, b_h, b_w, p_c, c)$:\n",
    " - $b_x, b_y, b_w, b_h$ sont les coordinées, la largeur, la hauteur d'une bbox,\n",
    " - $p_c$ est la probabilité de présence d'un objet,\n",
    " - $c$ est un vecteur de probabilité de classe à 80 dimensions (encodage `one-hot` pour un apprentissage qui a été fait sur 80 classes possible) ,\n",
    " - Chaque bbox est alors representée par 85 nombres. \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1dxMz5Ci_RASxBTBl5IzTzs9OedxGIFhu\" style=\"width:500px;height:250;\"><br>\n",
    "<caption> <b> Figure 1: Definition des paramètresde la boite englobante</b>  </caption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 - Entrées, sorties du modèle YOLOv3\n",
    "\n",
    "Le modèle YOLOv3 est un CNN composé d'un extracteur de features appelé Darknet-53 auquel on a rajouté 3 `têtes` de sortie. \n",
    "\n",
    "Voici quelques infos sur l'entrée et les sorties du modèle:\n",
    "- L'**entrée** est un batch (on utilise tensorflow!) de $m$ images de taille ($m$, 416, 416, 3),\n",
    "- La **sortie** est une **liste de listes de bbox**. Le nombre d'éléments du tuple de sortie est de 3, ce qui équivaut aux possibilités de détection à petite, moyenne et grande échelles de l'algorithme Yolov3. Pour chaque tuple d'échelle, une liste de bbox avec les classes reconnues est produite. \n",
    "\n",
    "En observant des images dans un contexte de détection d'objets, on peut remarquer que:\n",
    "- un objet (de la même classe ou de classes différentes) peut être observé à des tailles différentes,\n",
    "- des objets puissent être localisés à peu près au même endroit dans l'image.\n",
    "\n",
    "De façon à mieux prendre en compte la différence de taille des objets dans l'image, l'algorithme Yolov3 utilise 3 échelles de détection. Pour chacune des échelles (petite, moyenne, grande), le modèle sort une carte d'activation (i.e. `feature map`) dont la dimension dépend de l'échelle. \n",
    "\n",
    "De façon à pouvoir détecter la présence d'objets spatialement proche, YOLOv3 predits dans ces échelles des offsets par rapport 3 bbox prédéfinies et de taille différente.\n",
    "\n",
    "Ainsi, si l'entrée est une image de taille ($m$, 416, 416, 3), la dimension de la sortie pour la:\n",
    "- petite échelle (facteur de réduction 8) est ($m$, 52, 52, 3, 85)\n",
    "- échelle moyenne (facteur de réduction 16) est ($m$, 26, 26, 3, 85)\n",
    "- grande échelle (facteur de réduction 32) est ($m$, 13, 13, 3, 85)\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1nB-K-2tfxWxTx0zVKMBQ2J-pS2B8coEO\" style=\"width:700px;height:400;\"><br>\n",
    "   <caption> \n",
    "   <b>Figure 2 : Illustration de l'encodage YOLOv3 d'une image pour la grande échelle </b> \n",
    "    </caption>\n",
    "</center>\n",
    "\n",
    "Au final, YOLOv3 analyse (52\\*52+26\\*26+13\\*13)\\*3=10647 bbox pour une seule image pour détecter la présence d'object et les classer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tester YOLOv3 avec un modèle préentrainé sur des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette partie est d'appliquer le modèle YOLOv3 préentrainé sur la base d'images [coco](https://cocodataset.org/) sur des images de taille quelconque.\n",
    "\n",
    "La base d'images [coco](https://cocodataset.org/) regroupe des images de 80 classes:\n",
    "person, bicycle, car, motorbike, aeroplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, etc.\n",
    "Les noms des classes sont stockés dans le fichier `coco_classes.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Définition des classes\n",
    "\n",
    "**Question 4.1: Charger les noms des classes à partir du fichier `coco_classes.txt` présent dans le répertoire data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# class_names = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "print('nombre de classes dans la base d''images coco: {}'.format(len(class_names)))\n",
    "print(class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fonction d'affichage pour l'affichage de bbox\n",
    "\n",
    "La fonction ci-dessous vous est fournie et vous servira plus tard pour afficher sur une image les bbox. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbox(image, bboxes, CLASSES=class_names, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
    "    '''\n",
    "     image: numpy array\n",
    "     bboxes: rectangular bbox in a 6D-array with (xmin, ymin, xmax, ymax,score, class_ind) shape\n",
    "     class_names: list of class_name (string)\n",
    "    '''\n",
    "    \n",
    "    # local variables\n",
    "    NUM_CLASS = len(CLASSES)\n",
    "    image_h, image_w, _ = image.shape\n",
    "    hsv_tuples = [(1.0 * x / NUM_CLASS, 1., 1.) for x in range(NUM_CLASS)]\n",
    "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        \n",
    "        # current bbox\n",
    "        coor = np.array(bbox[:4], dtype=np.int32)\n",
    "        score = bbox[4]\n",
    "        class_ind = int(bbox[5])\n",
    "        (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "        #  display option\n",
    "        bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
    "        bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
    "        if bbox_thick < 1: bbox_thick = 1\n",
    "        fontScale = 0.75 * bbox_thick\n",
    "\n",
    "        # put object rectangle on image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
    "\n",
    "        # put text label on image\n",
    "        if show_label:\n",
    "            # get text label\n",
    "            score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n",
    "\n",
    "            if tracking: score_str = \" \"+str(score)\n",
    "\n",
    "            try:\n",
    "                label = \"{}\".format(CLASSES[class_ind]) + score_str\n",
    "            except KeyError:\n",
    "                print(\"You received KeyError, this might be that you are trying to use yolo original weights\")\n",
    "                print(\"while using custom classes, if using custom model in configs.py set YOLO_CUSTOM_WEIGHTS = True\")\n",
    "\n",
    "            # get text size\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                                                                  fontScale, thickness=bbox_thick)\n",
    "            # put filled text rectangle\n",
    "            cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
    "\n",
    "            # put text above rectangle\n",
    "            cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "                        fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Création et chargement du modèle préentrainé\n",
    "\n",
    "Apprendre un modèle YOLOv3 from scratch prend un très long temps et nécessite une grande base de données composée d'images labelisées avec des boites englobantes pour des classes en grand nombre. \n",
    "\n",
    "\n",
    "**Question 4.2.1: chargez un modèle déjà préentrainé sur la base coco dont les poids sont enregistrés dans le fichier `yolov3.weights` du répertoire data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille des images attendues par Yolo\n",
    "YOLO_INPUT_SIZE = 416\n",
    "\n",
    "# Chemin vers le fichier contenant les poids préentrainés\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# YOLO_V3_WEIGHTS = '...' # chemin vers yolov3.weights\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "from yolov3.yolov3 import Create_Yolov3\n",
    "from yolov3.utils import load_yolo_weights\n",
    "\n",
    "# Création de l'architecture dy modèle darknet 53 x\n",
    "yolo_model = Create_Yolov3(input_size=YOLO_INPUT_SIZE)\n",
    "\n",
    "# Création du modèle darknet 53 et chargement des poids préentrainés\n",
    "load_yolo_weights(yolo_model, YOLO_V3_WEIGHTS) \n",
    "\n",
    "print('YOLOv3 Model done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2.2: Affichez l'architecture du modèle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Your commented code below_`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# ...\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2.3: A l'aide de texte:** \n",
    "- **Décrivez l'architecture, cherchez et inclure une image de cette architecture**\n",
    "- **Donnez le nombre de paramètres entrainable de ce modèle**\n",
    "- **Trouvez dans le code où $b_x, b_y, b_w, b_h, conf, proba$ sont calculés en sachant que les sorties du réseau ne sont pas exactement $b_x, b_y, b_w, b_h, conf, proba$. Expliquez.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    " \n",
    "**Solution**\n",
    "- CNN: darknet avec 3 tetes de sortie des sous-échantillonnge 8,16,32\n",
    "- sorties en fait $t_x, t_y, t_w, t_h$ \n",
    "- $b_x, b_y, b_w, b_h, conf, proba$ calculés dans yolov3.decode\n",
    "- Total params: 62,001,757\n",
    "- Trainable params: 61,949,149\n",
    "- Non-trainable params: 52,608\n",
    "</FONT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Chargement d'une image avec opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path   = \"data/soireefoyz-bar.jpg\"\n",
    "original_image  = cv2.imread(image_path)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    cv2_imshow(cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "else:\n",
    "    cv2.imshow(\"original image\", cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "    # Load and hold the image\n",
    "    cv2.waitKey(0)\n",
    "  \n",
    "    # To close the window after the required kill value was provided\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Appliquer le modèle YOLOv3 à l'image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 - Prétraitements de l'image\n",
    "\n",
    "Le modèle YOLOv3 attend des images par batch ($m$, 416,416,3) en entrée. Il faut donc les transformer (réduire) à la taille adéquate en gardant le rapport hauteur/largeur. Il s'agit donc de:\n",
    "- déterminer le coefficient d'échelle pour réduire l'image originale \n",
    "- effectuer la réduction avec la fonction `resize` d'opencv\n",
    "- rajouter des pixels factices de valeurs  (128.0) pour compléter une image finale de 416x416\n",
    "\n",
    "**Question 4.4.1.1: compléter le code ci-dessous pour réaliser ces étapes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_h, yolo_w      = [YOLO_INPUT_SIZE,YOLO_INPUT_SIZE]\n",
    "orig_h,  orig_w, _  = original_image.shape\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# déterminer le coefficient d'échelle pour réduire l'image originale \n",
    "\n",
    "\n",
    "# resize_ratio = ...\n",
    "# new_w, new_h = ...\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# effectuer la réduction avec la fonction `resize` d'opencv\n",
    "\n",
    "# resized_image = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# créez une image de la taille finale 416x416\n",
    "# insérez l'image réduite\n",
    "# rajouter des pixels factices de valeurs  (128.0) pour compléter une image finale de 416x416\n",
    "\n",
    "# yolo_image = ...\n",
    "# diff_w, diff_h  = ...\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4.1.2: formez un batch d'images à une seule image normalisez entre [0,1] de type float32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# normalisez l'image strictement entre 0 et 1 \n",
    "\n",
    "# yolo_image = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# Rajout (np.newaxis) de la dimension batch à l'image avant car les modèles tensorflow le nécessitent\n",
    "\n",
    "# yolo_image = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "# on assure le type float32\n",
    "yolo_image = yolo_image.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4.1.3: Pourquoi doit-on faire cela?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    " \n",
    "**Solution**\n",
    "- Normalisation pour rentrer dans un réseau de neurones, pour suivre le modèle YOLOv3\n",
    "- Dimension batch à rajouter pour pouvoir utiliser tensorflow (pas moins de 4D)\n",
    "- float32 pour les calculs dans tensorflow \n",
    "\n",
    "</FONT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Afficher l'image résultante**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google colab nécessite une fonction spéciale pour l'affichage\n",
    "if IN_COLAB:\n",
    "  cv2_imshow(cv2.cvtColor(yolo_image[0,...]*255, cv2.COLOR_RGB2BGR))\n",
    "else:\n",
    "\n",
    "  cv2.imshow(\"Image réduite et avec rajout de valeurs\", cv2.cvtColor(yolo_image[0,...], cv2.COLOR_RGB2BGR))\n",
    "  cv2.waitKey(0)\n",
    "\n",
    "  cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 - Appliquer le modèle YOLOv3 (forward propagation)\n",
    "\n",
    "**Question 4.4.2.1: réaliser le code correspondant**\n",
    "\n",
    "**Attention: deux techniques sont possibiles avec tensorflow: mettre l'image en entrée du modèle ou utiliser la méthode 'predict' du modèle. Malheureusement, ces deux techniques ne sortent pas la même objet. Attention!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# bbox_pred = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "print('Predictions done')\n",
    "print('Dimensions de bbox_pred:')\n",
    "print([bbox.shape for bbox in bbox_pred])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4.2.2: Décrire tous les éléments de la variable `bbox_pred`. A quoi cela correspond?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Your commented code below_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "print('Sorties:')\n",
    "print('small scale: {}'.format(bbox_pred[0].shape))\n",
    "print('medium scale: {}'.format(bbox_pred[1].shape))\n",
    "print('large scale: {}'.format(bbox_pred[2].shape))\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Double cliquez ici pour écrire votre réponse ici!_**\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    " \n",
    "**Solution**\n",
    "Liste de 3 tenseurs liés aux 3 échelles:\n",
    "- small scale: 1 batch, grilles de 52x52, 3 bboxes, 85 paramètres: 4 de position, 1 objectness, 80 classes\n",
    "- medium scale : 1 batch, grilles de 26x26, 3 bboxes, 85 paramètres: 4 de position, 1 objectness, 80 classes\n",
    "- large scale : 1 batch, grilles de 13x13, 3 bboxes, 85 paramètres: 4 de position, 1 objectness, 80 classes\n",
    "    \n",
    "</FONT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Post traitement des sorties\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 - Mise en forme de `bbox_pred`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons réaliser ces post-traitement en numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_pred = [np.array(x) for x in bbox_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps d'implémenter une fonction prenant la sortie du CNN profond (l'encodage dimensionnel [[52x52x3x85], [26x26x3x85], [13x13x3x85]]) remodelé comme un tenseur [10647x85] avec toutes les bbox avant de  filtrer des bbox . \n",
    "\n",
    "\n",
    "**Question 4.5.1.1. Ecrire un code permettant:**\n",
    "- **de mettre en forme 'bbox_pred' en 2D avec 1 bbox par ligne et 85 descripteurs en colonne**\n",
    "- **séparer les colonnes de 'bbox_pred' en 3 matrices correspondant à xywh, conf (confidence score, objectness) et prob (probabilité de chaque porentiel objet/classe)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# mettre en forme 'bbox_pred' en 2D avec 1 bbox par ligne et 85 descripteurs en colonne (tensor shape of 10647x85)\n",
    "\n",
    "#     bbox_pred = ...\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "print('bbox_pred: {}'.format(bbox_pred.shape))\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# séparer les informations de bbox_pred\n",
    "\n",
    "#     bbox_pred_xywh = ...\n",
    "#     bbox_pred_conf = ...\n",
    "#     bbox_pred_prob = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 - Traitements des positions de `bbox_pred`\n",
    "\n",
    "**Question 4.5.2.1: Ecrire un code permettant de calculer xmin, ymin, xmax, ymax (nécessaire en particulier à `draw_bbox`) et de les mettre dans une matrice 10647 lignes et 4 colonnes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# convert (x, y, w, h) --> bbox_pred_coor = (xmin, ymin, xmax, ymax)\n",
    "\n",
    "#     bbox_pred_coor = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "print(bbox_pred_coor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5.2.2. Ecrire un code permettant de transformer les coordonnées (xmin, ymin, xmax, ymax) des bbox de l'image liées aux dimensions 416x416 vers les coordonnées liées la taille de l'image originale**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Attention**: Les coordonnées de la bbox à ce stade sont référencées par rapport à des dimensions 416x416. \n",
    "# Lors de la mise à cette dimension de l'image, pour garder le rapport h/w de l'image, il a fallu rajouter des lignes \n",
    "# et des colonnes (padding). Il s'agit donc ici de coder la transformation dans l'ordre inverse:\n",
    "# - se ramener aux coordonnées avant padding\n",
    "# - appliquer le rapport de dimensions \n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "#     bbox_pred_coor = ...\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 - Traitements des scores de `bbox_pred`\n",
    "\n",
    "**Question 4.5.3.1: Ecrire un code permettant de:**\n",
    "- **d'extraire l'indice `bbox_pred_class` et la probabilité `bbox_pred_class_prob` de la classe prédite pour chaque bbox**\n",
    "- **de calculer le score `bbox_pred_score` pour chaque bbox définit par:**\n",
    "\n",
    "**\\begin{align*}\n",
    "\tbbox\\_pred\\_score &= P[\\text{object in img}] \\times \\max_i{P[c_i|\\text{object in img}]} \n",
    "\\end{align*}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "#  extraire l'indice bbox_pred_class et la probabilité max bbox_pred_class_prob de la classe prédite pour chaque bbox\n",
    "\n",
    "# bbox_pred_class = ...\n",
    "# bbox_pred_class_prob = ...\n",
    "    \n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# calculer le score `bbox_pred_score` pour chaque bbox (cf slide 36 boxes_score)\n",
    "\n",
    "# bbox_pred_score = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lignes suivantes permettent d'afficher les résultats de prédiction. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_pred_tmp = np.concatenate([bbox_pred_coor, \n",
    "                            bbox_pred_score[:, np.newaxis], \n",
    "                            bbox_pred_class[:, np.newaxis]], axis=-1)\n",
    "\n",
    "bbox_pred_tmp = np.array(bbox_pred_tmp)\n",
    "\n",
    "\n",
    "img = draw_bbox(cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR), bbox_pred_tmp, CLASSES=class_names)\n",
    "    \n",
    "if IN_COLAB:\n",
    "  cv2_imshow(img)\n",
    "else:\n",
    "  cv2.imshow(\"image with detected objects\", img)\n",
    "  # Load and hold the image\n",
    "  cv2.waitKey(0)\n",
    "  cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5.3.2: Conclusion?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Double cliquez ici pour écrire votre réponse ici!_**\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    " \n",
    "**Solution**\n",
    "Il faut filtrer    \n",
    "           \n",
    "</FONT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 - Filtrage des `bbox`\n",
    "Il s'agit maintenant après tout ce travail préliminaire de filtrer les bbox à l'aide de maques permettant de sélectionner uniquement les meilleures. 3 étapes de filtrages vont être réalisées:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.6.1 - Masque (matrice booléenne) de filtrage des `bbox` de coordonnées erronnées\n",
    "\n",
    "**Question 4.6.1.1: Ecrire un code permettant d'obtenir un masque booléen (ou les indices correspondant aux `true`) tel que masque = 1:**\n",
    "- **si xmin,ymin >= 0**\n",
    "- **si xmax,ymax  <= tailles de l'image originale**\n",
    "- **et que xmin < xmax**\n",
    "- **et que ymin < ymax**\n",
    "\n",
    "Indication: `np.logical_and` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "# clip some boxes those are out of range\n",
    "\n",
    "# bbox_pred_valid_range_mask = ...\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "print('Pourcentage de bboxes garder: {}'.format(np.sum(bbox_pred_valid_range_mask)/bbox_pred_valid_range_mask.shape[0]))\n",
    "print('Expected values: 0.64778811')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 - Filtrage des `bbox` avec peu de confiance dans la présence d'un objet\n",
    "**Question: Ecrire un code permettant d'obtenir un masque booléen (ou les indices correspondant aux `true`) tel que masque = 1 si `box_score > BOX_SCORE_THRESHOLD`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_SCORE_THRESHOLD = 0.3\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# discard boxes with low scores\n",
    "# Créer un masuqe de filtrage basé syr kes scores et en utilisant le seuil ci-dessus.\n",
    "\n",
    "# bbox_pred_valid_score_mask=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "print('Pourcentage de bbox garder: {}'.format(np.sum(bbox_pred_valid_score_mask)/bbox_pred_valid_score_mask.shape[0]))\n",
    "print('Expected values: 0.00638678')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 - Filtrage des `bbox` avec les masques de filtrage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# Appliquer les masques to scores, boxes and classes\n",
    "\n",
    "# bbox_pred_coor, bbox_pred_score, bbox_pred_class = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# Filtrage et reconstruction de bbox_pred\n",
    "\n",
    "# bbox_pred = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.3 - Filtrage des `bbox` trop proches spatialement pour le même objet (Non-Max-Supression)\n",
    "\n",
    "Même après un filtrage par seuillage sur les scores des classes, on se retrouve avec un grand nombre de boîtes qui se chevauchent (Figure 2). \n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1szJubhI0n1dYLAZH5dyHe3MhJ4cRjDM3\" style=\"width:500px;height:400;\"><br>\n",
    "<caption> <b> Figure 3: Dans cet exemple, le modèle a prédit la présence de 3 voitures, mais il s'agit en fait de 3 prédictions de la même voiture. L'exécution de la suppression non maximale (NMS) sélectionnera uniquement la plus précise (probabilité la plus élevée) des 3 boîtes. </b> </caption>\n",
    "</center>\n",
    "\n",
    "Un deuxième filtre pour sélectionner les bonnes boîtes est appelé suppression non maximale (NMS). Ce filtre sélectionnera une seule bbox lorsque plusieurs bbox se chevauchent et détectent le même objet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3.1 - Préambule: Intersection over Union (IoU)\n",
    "\n",
    "Nous allons ici définir le moyen de quantifier le chevauchement de deux bbox par une métrique appelée Intersection over Union (IoU).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?id=1Li2JQY4bHLRVytQcV-iTxJaZ7v0HgNCB\" style=\"width:500px;height:400;\">\n",
    "<caption> <b> Figure 4 : Definition de l'\"Intersection over Union\". </br> </caption>\n",
    "</center>\n",
    "\n",
    "\n",
    "**Question: Ecrire le code permettant d'implémenter la fonction `cpt_bbox_iou` ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Your commented code below`_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpt_bbox_iou(boxes1, boxes2):\n",
    "    \n",
    "    \"\"\"Implement the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "    Arguments:\n",
    "    boxes1 -- first box, list object with coordinates (x1, y1, w1, h1)\n",
    "    boxes2 -- second box, list object with coordinates (x2, y2, w2, h2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Guaranti d'avoir du numpy array\n",
    "    boxes1 = np.array(boxes1)\n",
    "    boxes2 = np.array(boxes2)\n",
    "    \n",
    "    # Calculer les coordonnéees de l'intersection des box1 and box2. Calculer sa surface.\n",
    "    left_up       = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down    = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "    inter_area    = inter_section[..., 0] * inter_section[..., 1]\n",
    "\n",
    "\n",
    "    \n",
    "    # ----------- Your code here (≈ 3 lines) --------------------->\n",
    "    # Calculer la surface de l'union par la formule: Union(A,B) = A + B - Inter(A,B)\n",
    "    \n",
    "    # boxes1_area = ...\n",
    "    # boxes2_area = ...\n",
    "    # union_area =...\n",
    "\n",
    "    # ------------------------------------------------------------>\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # ----------- Your code here (≈ 1 lines) --------------------->\n",
    "    # Calculer l' IoU\n",
    "\n",
    "    # iou = ...\n",
    "\n",
    "    # ------------------------------------------------------------>\n",
    "\n",
    "\n",
    "    # éviter les 0s\n",
    "    ious          = np.maximum(iou, np.finfo(np.float32).eps)\n",
    "    \n",
    "    return ious\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test de la fonction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1 = np.array((2., 1., 4., 3.))\n",
    "box2 = np.array((1., 2., 3., 4.))\n",
    "a = cpt_bbox_iou(box1, box2)\n",
    "print(\"iou = \" + str(cpt_bbox_iou(box1, box2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **iou = **\n",
    "        </td>\n",
    "        <td>\n",
    "           0.14285714285714285\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.3.2 -  Non-Max-Suppression\n",
    "\n",
    "Vous êtes maintenant prêt à mettre en œuvre l'algorithme NMS. Il s'agit d'un algorithme itératif dontes étapes clés sont les suivantes : \n",
    "1. Sélectionnez toutes les bbox prédisants une classe particulière\n",
    "2. Sélectionnez celle qui a le score le plus élevé.\n",
    "3. Calculez son chevauchement avec les autres bbox, et supprimez les bbox dont le chevauchement est supérieur à `IOU_THRESHOLD`.\n",
    "4. Retournez à l'étape 1 et itérez jusqu'à ce qu'il n'y ait plus de bbox avec un score inférieur à celui de la boîte sélectionnée.\n",
    "\n",
    "Ceci éliminera toutes les bbox qui ont un grand chevauchement avec les bbox sélectionnées. Il ne reste que les \"meilleures\" bbox.\n",
    "\n",
    "**Question** : Complétez le code suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'algorithme NMS est ainsi:\n",
    "# boucles sur les classes prédites \n",
    "    # etape 1: selectionner les bboxes de la classe prédite courante (cls_bboxes)\n",
    "    # Tant que cette liste de bboxes (cls_bboxes) n'est pas vide\n",
    "        # Etape 2: selectionner la bbox 'best_bbox' avec le plus haut score dans cls_bboxes et la mettre dans la liste des bboxes finales best_bboxes et l'enlever de la liste cls_bboxes\n",
    "        # Etape 3: calculer l'IoU entre best_bbox et cls_bboxes\n",
    "        # Etape 4: Toutes les bboxes de cls_bboxes ayant un \\textbf{chevauchement (IoU) supérieur à un seuil prédéfini IOU_THRESHOLD(souvent 0.45)} avec la box best_bbox sont éliminées de cls_bboxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# indices de classe prédits dans l'images\n",
    "classes_in_img = list(set(bbox_pred[:, 5]))\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# ------------------------------------------------>\n",
    "print('Nombre de bboxes conservées: {}'.format(len(best_bboxes)))\n",
    "print('Valeur attendue: 35')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Afficher toutes les bboxes prédites sur l'image à l'aide de draw_bbox** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Your commented code below`_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bboxes = np.array(best_bboxes)\n",
    "\n",
    "\n",
    "img = draw_bbox(cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR), best_bboxes, CLASSES=class_names)\n",
    "    \n",
    "if IN_COLAB:\n",
    "  cv2_imshow(img)\n",
    "else:\n",
    "  cv2.imshow(\"image with detected objects\", img)\n",
    "  # Load and hold the image\n",
    "  cv2.waitKey(0)\n",
    "  cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Créer une image représentant un diagramme concernant le processus utilisé par YOLOv3 pour détecter et localiser des objets dans une image** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour insérer votre image ici!`_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tester YOLOv3 avec un modèle préentrainé pour la détection d'objects d'une voiture robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Créer scripts et fonctions permettant de lire, prédire et sauver les prédictions du répertoire drive**\n",
    "\n",
    "**Indication:**\n",
    "image_path = \"./data/drive/*.jpg\"\n",
    "img_array = []\n",
    "for name in glob.glob(image_path):\n",
    "\n",
    "  # chargement\n",
    "  original_image = cv2.imread(name)\n",
    "  original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  ...# pretraiter, predire, post traiter, inscriver les boites dans l'image courante \n",
    "\n",
    "  img_array.append(img_with_bboxes)\n",
    "\n",
    "\n",
    "# sauver les images dans un fichier .avi\n",
    "out = cv2.VideoWriter('detectcar.avi', cv2.VideoWriter_fourcc(*'DIVX'), 1, size)\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**: The ideas presented in this notebook came primarily from the two YOLO papers. The implementation here also took significant inspiration and used many components from Allan Zelener's github repository. The pretrained weights used in this exercise came from the official YOLO website. \n",
    "- Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)\n",
    "- Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)\n",
    "- Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)\n",
    "- The official YOLO website (https://pjreddie.com/darknet/yolo/) \n",
    "- [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OMdut",
   "launcher_item_id": "bbBOL"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
