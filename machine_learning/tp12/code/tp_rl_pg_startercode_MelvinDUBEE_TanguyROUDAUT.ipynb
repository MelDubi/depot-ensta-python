{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiV45FiTfw33"
   },
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font>   |\n",
    "|:-------------:     |:-----------   |:------  |\n",
    "| binôme 1  | <span style=\"color:red\">ROUDAUT</span> | <span style=\"color:red\">Tanguy</span>     |\n",
    "| binôme 2         | <span style=\"color:red\">DUBEE</span> | <span style=\"color:red\">Melvin</span>     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ECpY6qw1-6e"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# <center> **Reinforcement Learning with a policy gradient algorithm** </center>\n",
    "## <center> Machine Learning Programming Exercise 12</center>\n",
    "\n",
    "\n",
    "Ce TP concerne la découverte de l'apprentissage par renforcement appliqué au domaine du gaming. Les observations seront données par un émulateur de jeux atari. L'objectif de ce tp est la mise en oeuvre de différents algorithmes de type **policy gradient**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkW4S14Rfw35"
   },
   "source": [
    "# 1. Import useful packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoxZkBYefw36"
   },
   "source": [
    "## 1.1 Colab or not colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1UsL1Fsfw36",
    "outputId": "4d52609c-ea00-44b6-ed56-94314da71a71"
   },
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "\n",
    "  ipynb_name = 'tp_rl_pg_solution.ipynb'\n",
    "  # ipynb_name = 'tp_rl_pg_startercode.ipynb'\n",
    "\n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2gmEhnhefgk"
   },
   "source": [
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7u_UjGpeeoy",
    "outputId": "92dc45a7-4a47-4ec6-91ef-b6cb5c66db1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. CNNs can be very slow without a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: gymnasium[classic_control]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: gymnasium[atari]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: no matches found: gymnasium[accept-rom-license]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvirtualdisplay in /Users/tanguyroudaut/depot/depot-ensta-python/venv/lib/python3.11/site-packages (3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "\n",
    "# display imports\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "matplotlib.rc('animation', html='jshtml') # To get smooth animations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ML imports\n",
    "import tensorflow as tf\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "elif len(tf.config.list_physical_devices('GPU')) > 1:\n",
    "  # a décommenter si problème avec le GPU de votre machine\n",
    "  physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "  for gpu in physical_devices:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "# RL imports:  install specific packages for openai/gym envs\n",
    "\n",
    "# install gym\n",
    "%pip install -q -U gymnasium\n",
    "%pip install -q -U gymnasium[classic_control]\n",
    "%pip install -q -U gymnasium[atari]\n",
    "%pip install -q -U gymnasium[accept-rom-license]\n",
    "%pip install pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZurpHGP8eicW"
   },
   "source": [
    "## 1.3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "gjiaQejqee2S"
   },
   "outputs": [],
   "source": [
    "# initialisation des graines aléatoires\n",
    "SEED = 333\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAfJJURZfw39"
   },
   "source": [
    "## 1.4 some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nBD7vd8sfw39"
   },
   "outputs": [],
   "source": [
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V34jUIHfw3-"
   },
   "source": [
    "# 2. Introduction to OpenAI gym (facultatif si vous l'avez déjà fait)\n",
    "\n",
    "C'est le même que dans le tp sur les algorithmes `Q_learning`. Si vous l'avez déjà fait, passez directement à la section suivante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jxdgagWfw3_"
   },
   "source": [
    "Dans ce notebook, l'environnement et les observations/données seront fournis par le framework [OpenAI gym](https://gym.openai.com/). Il fournit de nombreux environnements avec lesquels votre *agent* peut interagir et apprendre une policy.\n",
    "L'import se fait classiquement par:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "QNPJDmytfw4A",
    "outputId": "5e58d7de-2f52-4431-ff73-dd02a01c70cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.29.1'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import gym\n",
    "import gymnasium as gym\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfE8IhFofw4A"
   },
   "source": [
    "La liste des environnements disponibles est donnée par:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FjGi_X0fw4C",
    "outputId": "29f8288b-7c7e-4860-f44d-51a2e3931f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "phys2d/CartPole-v0\n",
      "phys2d/CartPole-v1\n",
      "phys2d/Pendulum-v0\n",
      "LunarLander-v2\n",
      "LunarLanderContinuous-v2\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v2\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "tabular/Blackjack-v0\n",
      "tabular/CliffWalking-v0\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n",
      "GymV21Environment-v0\n",
      "GymV26Environment-v0\n"
     ]
    }
   ],
   "source": [
    "g = gym.envs.registry\n",
    "for l in list(g):\n",
    "  print(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G78ccZG3fw4C"
   },
   "source": [
    "## 2.1 Le Cart-Pole\n",
    "\n",
    "Le [Cart-Pole](https://gym.openai.com/envs/CartPole-v1/) est un environnement très simple dans lequel un chariot peut bouger soit vers la gauche, soit vers la droite. Un bâton/piquet est placé sur ce dernier. L'agent doit alors bouger le chariot à gauche ou à droite pour que le bâton reste droit.\n",
    "<table align=\"center\">\n",
    "<td align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1RNWq3McGaTydH2PrNQWFcXLxV7Lg2mC8\" height=\"300px\" />\n",
    "<td/>\n",
    "</table>\n",
    "\n",
    "L'environnement est défini par la commande `make()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "D1uvHHOvfw4D"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V2m8E_afw4D"
   },
   "source": [
    "Il faut ensuite initialiser l'environnement en appelant la méthode `reset()` qui retourne la première observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEaK4-FWfw4D",
    "outputId": "fd847e3a-21c3-4887-bd7f-15aabf47c34e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.04190018, 0.03315072, 0.00547609, 0.03063206], dtype=float32), {})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 333\n",
    "obs = env.reset(seed=SEED)\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EiCjPuRfw4E"
   },
   "source": [
    "Les observations dépendent de l'environnement considéré. Dans ce cas, les observations sont regroupées dans un array Numpy 1D soit 1 vecteur composé de 4 floats. Ce vecteur regroupe:\n",
    "- la position horizontale du chariot (<0 gauche; 0 = verticale; >0 droite)\n",
    "- la vitesse du chariot  \n",
    "- l'angle du bâton (<0 gauche; 0 = verticale; >0 droite)\n",
    "- la vitesse angulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpFOwjxafw4E"
   },
   "source": [
    "Un environnement peut-être visualisé par appel à la méthode `render()`. Pour récupérer une observation (une image) de l'environnement sous la forme d'un array NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hh8MqjVNfw4F",
    "outputId": "355c1269-a53f-4f6a-f28f-631ab0a39e56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnMJml4Kfw4F"
   },
   "source": [
    "La fonction `plot_environment()` permet de récupérer et d'afficher l'observation courante de l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uiTffEIRfw4F"
   },
   "outputs": [],
   "source": [
    "def plot_environment(env, figsize=(5,4)):\n",
    "    # get an observation\n",
    "    img = env.render()\n",
    "\n",
    "    # display an observation\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ycD5hTs9fw4G",
    "outputId": "d98ff317-37c8-4322-856e-c61c6671e7e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_environment(env)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjVhMSeKfw4H"
   },
   "source": [
    "Pour interagir avec l'environnement, l'agent doit sélectionner une action à partir de l'espace des actions possibles. Pour chaque environnement, la commande `action_space` permet de connaitre cet espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdwl5fOufw4H",
    "outputId": "62925a43-ff0f-49b9-dd40-c16c214c89df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ReNiqlLfw4I"
   },
   "source": [
    "Pour le cartpole,deux actions sont possible: gauche ou droite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHytRxgffw4I"
   },
   "source": [
    "Dans l'état actuel, le bâton est penché vers la droite (`obs[2] > 0`); passons l'action de déplacer le chariot vers la droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lLBwzMKNfw4I",
    "outputId": "ed95755d-2b3d-44ed-a6bf-9519fc9b0184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12850673,  1.4006819 , -0.11356623, -2.0642416 ], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action\n",
    "go_right = True  # or 1, go right\n",
    "# go_right = False # or 0, go left\n",
    "\n",
    "# interaction with the environment\n",
    "obs, reward, done, truncated, info = env.step(go_right)\n",
    "\n",
    "# observation\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f7CBR0Lfw4I"
   },
   "source": [
    "Remarquez que:\n",
    "- le chariot se déplace maintenant vers la droite (`obs[1] > 0`);\n",
    "- le bâton est toujours incliné vers la droite (`obs[2] > 0`) ;\n",
    "- mais sa vitesse angulaire est maintenant négative (`obs[3] < 0`), il est ainsi vraissemblable qu'il soit incliné vers la gauche après la prochaine action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5ytl4ilTfw4J",
    "outputId": "bdf741e8-c195-4916-dc20-fdace2f70d87"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_environment(env);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dwqAsgGfw4K"
   },
   "source": [
    "L'interaction avec l'environnement:\n",
    "- génère l'observation suivante `obs`\n",
    "- renseigne l'agent sur la récompense générée par l'action prise: `reward`\n",
    "- retourne `done=True` si le jeu est fini (pour cet environnement, le jeu est fini si l'angle du baton est de plus de 12° ou si le chariot s'est déplacé de plus 2.4 unités par rapport au centre.)\n",
    "- `truncated`: cette valeur sera `True` lorsqu'un épisode est interrompu prématurément, par exemple par un wrapper d'environnement qui impose un nombre maximum d'étapes par épisode (voir la documentation de Gym pour plus de détails sur les wrappers d'environnement).\n",
    "- info est un dictionnaire spécifique à chaque environnement qui peut contenir des informations supplémentaires pour le debugging ou l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8737bESfw4K",
    "outputId": "1ef0bed2-50a0-41b4-86d0-c2312a918597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 False False\n"
     ]
    }
   ],
   "source": [
    "print(reward, done, truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX1xdMupfw4L"
   },
   "source": [
    "Comme nous l'avons vu en cours, un **épisode** correspond à la séquence d'interactions entre le moment où l'environnement est lancé `reset()` et le moment ou `done=True`. Pour relancer un épisode, il faut refaire appel à la méthode `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "JzELkvf7fw4L"
   },
   "outputs": [],
   "source": [
    "if done or truncated:\n",
    "  obs = env.reset(seed=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpHmxeZzfw4L"
   },
   "source": [
    "## 2.2 Autres environnement (facultatif)\n",
    "\n",
    "**Question:**\n",
    "- **Charger l'environnement SpaceInvaders-v0**\n",
    "- **Déterminer :**\n",
    "  - **l'espace d'actions**\n",
    "  - **l'espace d'observation**\n",
    "\n",
    "- **et créer une image/copie de chaque environnement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "F1QfGz--fw4M",
    "outputId": "59c12796-f78f-4759-ebb5-64f595068d92"
   },
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `SpaceInvaders` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tanguyroudaut/depot/depot-ensta-python/machine_learning/tp12/code/tp_rl_pg_startercode_MelvinDUBEE_TanguyROUDAUT.ipynb Cell 39\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguyroudaut/depot/depot-ensta-python/machine_learning/tp12/code/tp_rl_pg_startercode_MelvinDUBEE_TanguyROUDAUT.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# ----------- Your code here --------------------->\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tanguyroudaut/depot/depot-ensta-python/machine_learning/tp12/code/tp_rl_pg_startercode_MelvinDUBEE_TanguyROUDAUT.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m space_invaders \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mSpaceInvaders-v0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguyroudaut/depot/depot-ensta-python/machine_learning/tp12/code/tp_rl_pg_startercode_MelvinDUBEE_TanguyROUDAUT.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m action_space \u001b[39m=\u001b[39m space_invaders\u001b[39m.\u001b[39maction_space\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tanguyroudaut/depot/depot-ensta-python/machine_learning/tp12/code/tp_rl_pg_startercode_MelvinDUBEE_TanguyROUDAUT.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m observation_space \u001b[39m=\u001b[39m space_invaders\u001b[39m.\u001b[39mobservation_space\n",
      "File \u001b[0;32m~/depot/depot-ensta-python/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:741\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39mstr\u001b[39m)\n\u001b[1;32m    740\u001b[0m     \u001b[39m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     env_spec \u001b[39m=\u001b[39m _find_spec(\u001b[39mid\u001b[39;49m)\n\u001b[1;32m    743\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[1;32m    745\u001b[0m \u001b[39m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[0;32m~/depot/depot-ensta-python/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:527\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m    521\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    522\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the latest versioned environment `\u001b[39m\u001b[39m{\u001b[39;00mnew_env_id\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    523\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead of the unversioned environment `\u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m env_spec \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     _check_version_exists(ns, name, version)\n\u001b[1;32m    528\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    529\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo registered env with id: \u001b[39m\u001b[39m{\u001b[39;00menv_name\u001b[39m}\u001b[39;00m\u001b[39m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    532\u001b[0m \u001b[39mreturn\u001b[39;00m env_spec\n",
      "File \u001b[0;32m~/depot/depot-ensta-python/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:393\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m get_env_id(ns, name, version) \u001b[39min\u001b[39;00m registry:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m _check_name_exists(ns, name)\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/depot/depot-ensta-python/venv/lib/python3.11/site-packages/gymnasium/envs/registration.py:370\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m namespace_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in namespace \u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m ns \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m suggestion_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Did you mean: `\u001b[39m\u001b[39m{\u001b[39;00msuggestion[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m`?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m suggestion \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 370\u001b[0m \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mNameNotFound(\n\u001b[1;32m    371\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnvironment `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist\u001b[39m\u001b[39m{\u001b[39;00mnamespace_msg\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00msuggestion_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `SpaceInvaders` doesn't exist."
     ]
    }
   ],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "space_invaders = gym.make('SpaceInvaders-v0')\n",
    "action_space = space_invaders.action_space\n",
    "observation_space = space_invaders.observation_space\n",
    "\n",
    "print(\"Espace d'actions:\", action_space)\n",
    "print(\"Espace d'observation:\", observation_space)\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKX461TKfw4M"
   },
   "source": [
    "# 3. Définition d'une politique simple (policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBpGsQgLfw4N"
   },
   "source": [
    "Revenons au **CartPole** et essayons de maintenir le bâton droit. Nous avons besoin d'une policy (politique/stratégie) pour cela. Il s'agit d'une stratégie que l'agent utilisera pour sélectionner une action à chaque étape. Cette stratégie pourra potentiellement utiliser toutes les actions et observations passées pour faire cela.\n",
    "\n",
    "Pour le moment, nous allons définir une politique très simple consistant à déplacer le chariot vers la gauche (resp. la droite) quand il penche vers la gauche (resp. la droite). Ici, la policy $\\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{o}_t)$ est déterministe pourra donc s'écrire $\\mathbf{a}_t = \\pi(\\mathbf{o}_t)$.\n",
    "\n",
    "**Question 3.1: Pour l'implémentation vous créerez:**\n",
    "- **une fonction `select_action_simple_policy(obs)` retournant l'action à prendre en fonction de l'observation**\n",
    "- **un script qui permet de**\n",
    " - **générer `N_EPISODES` (500) **épisodes** (ou **trajectoires**, **roll_out**)**\n",
    " - **chacun de ces épisodes comprend un nombre max d'interactions (de pas temporels) avec l'environnement fixé à `N_STEPS_MAX_PER_EPISODE = 200`**\n",
    " - **pour chaque interaction (pas temporel), décider de l'action à prendre (appel à `select_action_simple_policy()`), jouer cette action en interagissant avec l'environnement et récupérer la récompense et calculer la récompense totale**\n",
    " - **calculer les performances de l'algorithme (les statistiques: moyenne, std, min, max)**\n",
    "\n",
    "\n",
    "**Notes**: deux boucles imbriquées sont à coder, vous pouvez suivre le squellette suivant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJsPBrgNmDGG"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD393tKsfw4N"
   },
   "outputs": [],
   "source": [
    "\n",
    "# policy definition\n",
    "def select_action_simple_policy(obs):\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "    # go_right = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "    return go_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1ehE0FtRahH"
   },
   "outputs": [],
   "source": [
    "\n",
    "# parameters\n",
    "N_EPISODES = 500\n",
    "N_STEPS_MAX_PER_EPISODE = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hir4oNX5bdz6",
    "outputId": "d9cc34b3-f7cf-46c0-90d6-1406de52c681"
   },
   "outputs": [],
   "source": [
    "# boucle sur les épisodes/loop over the episodes\n",
    "episode_cumrewards_history=[]\n",
    "for episode in range(N_EPISODES):\n",
    "\n",
    "    # starting a new episode\n",
    "    episode_cumrewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "\n",
    "    # ...with N_STEPS_MAX_PER_EPISODE interactions (temps)\n",
    "    for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "      ## starting a new interaction ##\n",
    "\n",
    "      # select an action\n",
    "      # go_right = ...\n",
    "\n",
    "      # interaction with the environment\n",
    "      # obs...\n",
    "\n",
    "      # compute cumulative reward\n",
    "      episode_cumrewards += reward\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "      # continue or not\n",
    "      if done or truncated:\n",
    "        print('break at {}: done={},  truncated={}'.format(step, done, truncated))\n",
    "        break\n",
    "\n",
    "    # performance statistics\n",
    "    episode_cumrewards_history.append(episode_cumrewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68HwQYib418"
   },
   "source": [
    "**Question 3.2: Affichez:**\n",
    "- **la récompense cumulée moyenne +/- écart type calculée sur les épisodes**\n",
    "- **l'évolution de la récompense cumulée au cours des épisodes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4gmG7W1lBgc"
   },
   "source": [
    "**_`Your commented code below`_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "5FAYgD5Ufw4N",
    "outputId": "dc699338-a485-425e-8709-3868e6a4830c"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "# expected value 41.698 +/- 8.389445512070509\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Rwvn3BNfw4O"
   },
   "source": [
    "**Question 3.3**: Quelles sont vos conclusions sur les performances obtenues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlPNYB1_fw4O"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yUU9v0vfw4P"
   },
   "source": [
    "Vous pouvez visualiser l'évolution du cartpole au cours d'un episode avec la boucle suivante.\n",
    "\n",
    "**Question 3.4: Complétez les lignes demandées.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRfmEkrlfw4P",
    "outputId": "f2041661-d117-4b6f-9b39-8deabdf6f47b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialisation\n",
    "frames = []\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "# boucle temporelle d'interactions, un step = un pas temporel = une interaction\n",
    "for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "    # recuperation de l'environnement courant\n",
    "    img = env.render()\n",
    "    frames.append(img)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # selectionner une action\n",
    "    # go_right=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # interaction avec l'environnement\n",
    "    # obs...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # continue or not\n",
    "    if done or truncated:\n",
    "        print('break at {}: done={},  truncated={}'.format(step, done, truncated))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow3LHmoFfw4P"
   },
   "source": [
    "Vous pouvez définir un lecteur d'animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "MQv74_E-fw4Q",
    "outputId": "aa57c1d3-083f-41e5-e313-33fb6df03b21"
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zc6OFEVpfw4Q"
   },
   "source": [
    "# 4. Politiques par réseau de neurones (Neural Network Policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LHJsi5Cfw4R"
   },
   "source": [
    "Nous allons la remplacer la policy avec une stratégie trop simple par un réseau de neurones permettant plus de complexité.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaBfBQzN7Iw6"
   },
   "source": [
    "## 4.1 Création d'un réseau de neurones pour la policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RRwDwEA7EQB"
   },
   "source": [
    "**Question 4.1: Commencer par créer un model séquentiel de réseau de neurones à deux couches dense :**\n",
    "- **Ce réseau prendra une observation en entrée. Quelle est la taille des entrées?**\n",
    "- **La couche cachée sera de 5 neurones et l'activation de type `relu`.**\n",
    "- **Ce réseau prédira l'action à prendre pour chaque observation. Pour choisir l'action, le réseau estimera la probabilité d'aller à gauche pour chaque action.**\n",
    "- **Quel est le nombre de neurones de la couche de sortie? Quelle sera la fonction d'activation de la dernière couche? Pourquoi?**\n",
    "- **Comment prendre la décision d'aller à gauche ou à droite lors de la prochaine interaction?**\n",
    "\n",
    "**Notes sur observations et états**:\n",
    "Dans cet environnement particulier, les actions passées et les observations peuvent être ignorées car chaque observation contient l'état complet de l'environnement. Si par exemple, l'observation ne contenait pas la vitesse du chariot, on aurait besoin de quelques observations successives de la position du chariot pour l'estimer et répondre au problème. Si la qualité des observations était très mauvaises par exemple dans le cas de données bruitées, il faudrait probablement mettre dans les observations, quelques observations successives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBHRXetZfw4R"
   },
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6SnkFbjfw4R"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# n_inputs =...\n",
    "#policy_model=...\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqByeDlD_FEz"
   },
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_tPB8EO_EKw",
    "outputId": "7929648c-673c-405b-a2a5-47f2bcf1754e"
   },
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=SEED)\n",
    "print(obs)\n",
    "print(policy_model(obs[np.newaxis]))\n",
    "\n",
    "# expected outputs:\n",
    "# [0.04190018 0.03315072 0.00547609 0.03063206]\n",
    "# tf.Tensor([[0.500677]], shape=(1, 1), dtype=float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hKgyQSGfw4S"
   },
   "source": [
    "## 4.2 Sélectionner une action à partir de la policy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Question 4.2: Complétez la fonction `select_action_from_policy` qui sélectionne l'action et retourne la variable booléenne `go_right`**.\n",
    "\n",
    "\n",
    "**Nous allons distinguer deux modes de sélection de l'action.**\n",
    "- **si on est en mode `testing`, il s'agit uniquement d'utiliser l'action choisie à partir de la policy.**\n",
    "- **si on est en mode `training`, on doit définir une stratégie combinant exploration (aléatoire) de nouveau comportement et exploitation de la policy (action choisie à partir de la policy).**\n",
    "\n",
    "\n",
    "**Notes sur le compromis entre exploration et exploitation/action**\n",
    "Vous devez définir sous le commentaire `# exploration or exploitation/action strategy` une stratégie qui consiste à choisir entre l'action la plus probable et l'exploration aléatoire de l'espace d'actions. Cette approche laisse l'agent trouver le bon équilibre entre explorer de nouvelles (séquences d') actions et exploiter les actions qui sont connues pour bien fonctionner. Imaginer que vous alliez dans un restaurant pour la première fois et que tous les plats vous semblent de la même manière appétissants. Vous en choisissez un au hasard. S'il vous satisfait, vous allez augmenter la probabilité de le choisir à votre prochain passage dans ce restaurant. Si vous fixez cette probabilité à 1, vous n'essaierez jamais les autres plats...\n",
    "\n",
    "Ici une stratégie simple sera de :\n",
    "- réaliser un tirage aléatoire uniforme d'un réel compris entre 0 et 1. (essayer de fonctionne uniquement avec tensorflow)\n",
    "- vérifier si ce tirage est supérieur à la probabilité de déplacer le chariot vers la gauche, le résultat sera stocké dans la variable `go_right`;\n",
    "- `go_right` sera alors:\n",
    " - égale au booléen `False` avec la probabilité `left_proba` (le chariot ira à gauche);\n",
    " - ou égale au booléen `True` avec la probabilité `1 - left_proba` (le chariot ira à droite);\n",
    "- il suffit alors de \"caster\" ce booléen en entier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeEZosNiqFus"
   },
   "outputs": [],
   "source": [
    "def select_action(obs, mode='training'):\n",
    "\n",
    "\n",
    "  # estimer la probabilité d'aller à gauche à partir de l'observation à partir du modèle de policy\n",
    "  # Indications:\n",
    "  #     - ne pas utiliser la méthode .predict (qui renvoie une variable numpy) mais l'appel au model simple (qui renvoie une variable tensorflow)\n",
    "  #     - policy_model est un modèle tensorflow qui prend en entrée nécessairement un batch de d'entrées, pensez donc à augmenter la dimension des observations\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "  # left_proba = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # exploration or action strategy\n",
    "\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "\n",
    "  if mode =='training':\n",
    "\n",
    "    nb_rand = tf.random.uniform([1, 1])\n",
    "\n",
    "    go_right = (nb_rand > left_proba)\n",
    "  # go_right = ...\n",
    "\n",
    "  else:\n",
    "    go_right = (left_proba < 0.5)\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "  # cast en entier\n",
    "  go_right  = int(go_right)\n",
    "\n",
    "\n",
    "  return go_right, left_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKnnNg1S-zn5"
   },
   "source": [
    "**Tester la fonction par ce code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E34la9Sa-4cJ",
    "outputId": "a69a34ef-a61a-4456-9f82-2eb73007d497"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "go_right, left_proba = select_action(obs, mode='testing')\n",
    "print('output of policy_model: ', left_proba)\n",
    "print('go_right: ', go_right)\n",
    "\n",
    "go_right, left_proba = select_action(obs, mode='training')\n",
    "print('output of policy_model: ', left_proba)\n",
    "print('go_right: ', go_right)\n",
    "\n",
    "\n",
    "\n",
    "# Expected outputs (if you used tensorflow as random generator)\n",
    "# output of policy_model:  tf.Tensor([[0.500677]], shape=(1, 1), dtype=float32)\n",
    "# go_right:  0\n",
    "# output of policy_model:  tf.Tensor([[0.500677]], shape=(1, 1), dtype=float32)\n",
    "# go_right:  1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zTUz7Kl-7rb"
   },
   "source": [
    "## 4.3 Interagir avec l'environnement pour un épisode\n",
    "\n",
    "Créer le script qui vous permet de tester votre policy sur un épisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaMUilpUfw4S",
    "outputId": "8d234abb-1011-47ab-c844-73f2a5b0e1aa"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_STEPS_MAX_PER_EPISODE = 200\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5DMRI6-fw4S"
   },
   "source": [
    "On peut alors voir le déroulement d'un épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "EJCsd29Afw4T",
    "outputId": "29c75b1e-5410-45ce-99e2-612956518701"
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ8pCvOrfw4T"
   },
   "source": [
    "A ce stade, les poids du réseau de neurones sont aléatoires. Aucun apprentissage n'a eu lieu.\n",
    "L'algorithme REINFORCE de type policy gradient nous permettra de faire cet apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LsZ3_xTfw4T"
   },
   "source": [
    "# 5. Apprentissage par l'algorithme \"Policy Gradient\"\n",
    "\n",
    "\n",
    "## 5.1 Description de l'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9jNi-_mfw4T"
   },
   "source": [
    "Pour entraîner ce réseau de neurones, nous devrons définir une fonction de coût. Pour mémoire, l'expression théorique pour les algorithmes PG de la fonction de coût et de son gradient par rapport aux paramètres $\\theta$ :\n",
    "\t\\begin{align*}\n",
    "\t\tJ(\\theta) &= E_{\\tau \\sim p_\\theta(\\tau)}\\left[ R(\\tau)\\right]= E_{\\tau \\sim p_\\theta(\\tau)} \\left[\\sum\\limits_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_{t},\\mathbf{a}_{t}) \\right] \\\\\n",
    "\t\t\\nabla_{\\theta}J(\\theta) &= E_{\\tau \\sim p_\\theta(\\tau)} \\left[ \\sum\\limits_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{t}|\\mathbf{s}_{t}) \\cdot \\sum\\limits_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_{t},\\mathbf{a}_{t}) \\right] \\\\\n",
    "\t\\end{align*}\n",
    "\n",
    "L'algorithme _Policy Gradients_ s'attaque à ce problème en jouant (ou générant) d'abord $N$ épisodes avec la _policy_ courante à l'épisode $i$, $\\pi_{\\theta}(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t})$.\n",
    "\n",
    "La fonction de coût théorique et ses gradients peuvent alors être estimés (la moyenne étant l'estimateur naturel de l'espérance) à partir des $N$ épisodes par:\n",
    "\\begin{align}\n",
    "\t\tJ(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N  \\sum\\limits_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_{i,t},\\mathbf{a}_{i,t})\\\\\n",
    "\t\\nabla_{\\theta}J(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N \\left[ \\left(\\sum\\limits_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t})\\right) \\left( \\sum\\limits_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_{i,t},\\mathbf{a}_{i,t}) \\right) \\right]\n",
    "\\end{align}\n",
    "\n",
    "Grâce à ces estimations les poids du réseaux de neurones sont ajustés en rendant les actions des bons épisodes légèrement plus probables, tandis que les actions des mauvais épisodes sont rendues légèrement moins probables.\n",
    "\n",
    "\n",
    "L'algorithme d'apprentissage **REINFORCE** est alors:\n",
    "- Générer ou obtenir des $N$ épisodes $ \\{ \\tau^i\\}_{i=1,\\dots,N} $ selon la policy $\\pi_{\\theta}(\\mathbf{a}_t|\\mathbf{s}_t)$ ce qui permet d'obtenir les gradients ($\\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) $) par rapports aux paramètres du réseau pour tous les épisodes et toutes les interactions.\n",
    "- Effectuer la `gradient ascent` en deux phases\n",
    " - Evaluer les gradients de la fonction de couts:\n",
    "\\begin{align}\n",
    "\t\t\\nabla_{\\theta}J(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N \\left[ \\left(\\sum\\limits_{t=0}^{T-1} \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t})\\right) \\left( \\sum\\limits_{t=0}^{T-1} \\gamma^t r(\\mathbf{s}_{i,t},\\mathbf{a}_{i,t}) \\right) \\right]\n",
    "\t\t\\end{align}\n",
    " - Update the parameters $ \\theta \\leftarrow \\theta + \\eta \\nabla_{\\theta} J(\\theta) $\n",
    " - Réitérer les deux phases pendant plusieurs itérations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKL3oMwd0C93"
   },
   "source": [
    "## 5.2 Mise en oeuvre de l'algorithme\n",
    "\n",
    "Dans cette section, nous allons générer des données et effectuer une `gradient descent` de manière à apprendre une `policy` à partir de ces données. Dans les algorithmes de deep learning rencontré précedemment, la descente de gradient était simple à utiliser car tout était \"classique\". Dans la mise en oeuvre de l'algorithme REINFORCE, on va devoir programmer un peu plus à la main.\n",
    "\n",
    "\n",
    "La première étape consiste à récupérer pour chaque interaction (pas temporel), les récompenses obtenues et évaluer les termes $\\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) $. Dans une seconde étape, nous le ferons pour tous les $N$ épisodes ayant plusieurs interactions (pas temporels). La dernière étape, consistera à réaliser la `gradient descent` à la place d'une gradient ascent car les gradients que nous allons évaluer seront l'opposé de ce qu'on doit avoir.\n",
    "\n",
    "\n",
    "### 5.2.1 Evaluation des gradients $\\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) $ pour chaque interaction\n",
    "\n",
    "Nous voulons obtenir pour chaque interaction $ \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) $ en sachant que $\\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t})$ est donné par la sortie du réseau de neurones.\n",
    "\n",
    "Un moyen malin détourné d'obtenir ces gradients est d'utiliser la loss de cross entropy.\n",
    "Ici la sortie code deux classes uniquement (1 seul neurone), il s'agit alors d'utiliser la cross entropy binaire (BCE, `tf.keras.losses.binary_crossentropy`). Dans ce cas, la fonction de cout pour un échantillon $i$ est:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\tJ_{BCE}(\\theta) &= - y_{target}^{(i)} \\cdot \\log \\left( \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) \\right) - (1-y_{target}^{(i)}) \\cdot \\log \\left( 1 - \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) \\right)\n",
    "\t\\end{align*}\n",
    "  où: $y_{target}^{(i)}$ est ici une classe cible .\n",
    "\n",
    "Pour que les calculs de gradients nous fournissent les bonnes valeurs, il s'agit d'y avoir une correspondance entre l'action qu'on va choisir et la classe cible. Si l'action choisie est d'aller à gauche, il faut fixer cette classe cible à $y_{target} = 1$ et dans le cas contraire $y_{target} = 0$. Le gradient final sera calculé plus tard et en particulier sera calculé à partir de ces gradients pondérés par la qualité de l'action choisie quantifiée par la somme cumulée des récompenses (reward-to-go).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk15eueogD6D"
   },
   "source": [
    "### 5.2.2 Générer les données pour une interaction (un pas temporel)\n",
    "\n",
    "Auparavant, nous avons déjà codé une interaction et un épisode de plusieurs interactions. Il va falloir désormais intercaller le calcul de la cross entropy et celui des gradients par rapport aux paramètres du réseau de neurones.\n",
    "\n",
    "Les données les plus importantes que l'on veut récolter pour chaque itération sont la valeur de la loss et les gradients de la loss et d'autre part la récompense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vm4BAahrfw4U"
   },
   "source": [
    "**Question 5.1: Commençons donc par créer une fonction `run_one_learning_step` permettant de réaliser une seule interaction (pas temporel) à partir d'une observation:**\n",
    " - **en utilisant le modèle de réseau de neurones pour prédire la probabilité d'aller à gauche**\n",
    " - **en calculant la loss et ses gradients (nous allons simplement enregistrer ces gradients pour l'instant, et les modifier plus tard en fonction de la qualité des actions (jugée par les récompenses avec rabais par exemple))**\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- pour calculer un gradient avec Tensorflow (sans la fonction `fit` de Keras), il faut calculer la `loss` dans le contexte `tf.GradientTape()`. Les gradients sont ensuite calculés par la méthode `gradient` (voir le code ci-dessous).\n",
    "- `y_target` devra être égale à 1 si l'action est d'aller à gauche (=0) et 0 dans le cas contraire et de la même dimension que left_proba.\n",
    "- `tf.reduce_mean(loss_fcn(...)` permettra d'estimer la loss moyenne sur l'ensemble des batchs.\n",
    "- Attention aux types des différentes variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hVPYksufw4U"
   },
   "outputs": [],
   "source": [
    "def run_one_learning_step(env, obs, policy_model, loss_fn):\n",
    "\n",
    "    # Calculs des gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # select action\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "        # Calcul de la loss\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # gradient de la loss en fonction des paramètres entrainables du modèle\n",
    "    grads = tape.gradient(loss, policy_model.trainable_variables)\n",
    "\n",
    "    # interaction avec l'environnement\n",
    "    obs, reward, done, truncated, info = env.step(int(go_right))\n",
    "\n",
    "\n",
    "    # sorties\n",
    "    return obs, reward, done, truncated, grads, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeFH2g8Yh3RO"
   },
   "source": [
    "**Test de la fonction `run_one_learning_step`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUclUV3Dfw4V",
    "outputId": "82521784-9acf-4137-dd32-99d23db810dd"
   },
   "outputs": [],
   "source": [
    "# initialisation de l'environnement\n",
    "SEED = 33\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# on fixe les graines aléatoires\n",
    "np.random.seed(SEED)\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# choix de la fonction de coût\n",
    "loss_fcn = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "# run\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "step_obs, step_reward, step_done, step_truncated, step_grads, step_loss = run_one_learning_step(env, obs, policy_model, loss_fcn)\n",
    "\n",
    "# display results\n",
    "print('reward: {}'.format(step_reward))\n",
    "print('grads: {}'.format(step_grads))\n",
    "print('loss: {}'.format(step_loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Vérification\n",
    "# reward: 1.0\n",
    "# grads: [<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
    "# array([[-0.        , -0.00192214, -0.        ,  0.00083397,  0.        ],\n",
    "#        [ 0.        ,  0.00233596,  0.        , -0.00101352, -0.        ],\n",
    "#        [ 0.        ,  0.01391877,  0.        , -0.00603903, -0.        ],\n",
    "#        [-0.        , -0.00838155, -0.        ,  0.00363656,  0.        ]],\n",
    "#       dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
    "# array([ 0.        ,  0.34105957,  0.        , -0.14797775, -0.        ],\n",
    "#       dtype=float32)>, <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
    "# array([[-0.        ],\n",
    "#        [-0.00616049],\n",
    "#        [-0.        ],\n",
    "#        [-0.00111578],\n",
    "#        [-0.        ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.5019212], dtype=float32)>]\n",
    "# loss: 0.6969969272613525\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov5YxPUBtsHt"
   },
   "source": [
    "**Question: quelles sont les dimensions des sorties step_reward, step_grads, step_loss? Pourquoi?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA_dFuUnx2mW"
   },
   "source": [
    "**`Your answer here`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvLm3VS7hAD2"
   },
   "source": [
    "### 5.2.3 Générer les données pour tous les épisodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS2keNpFfw4V"
   },
   "source": [
    "**Question 5.2: Maintenant, créez une autre fonction qui s'appuiera sur la fonction `run_one_learning_step()` pour jouer plusieurs épisodes, en retournant toutes les récompenses et les gradients, pour chaque épisode et chaque step.**\n",
    "\n",
    "**Note: concaténer ces gradients**\n",
    "- dans une liste pour chaque step (itération)\n",
    "- et une liste de listes pour chaque épisode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo5L9banfw4W"
   },
   "outputs": [],
   "source": [
    "def run_episodes(env, n_episodes, n_steps_max, policy_model, loss_fcn):\n",
    "\n",
    "    # initialisation\n",
    "    episodes_reward = []\n",
    "    episodes_grads   = []\n",
    "    episodes_loss  = []\n",
    "\n",
    "    # episode loop\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        # initialisation\n",
    "        steps_reward = []\n",
    "        steps_grads   = []\n",
    "        steps_loss  = []\n",
    "\n",
    "        # recuperation de la première observation\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "        # step_obs, step_info = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "        # interaction (step) loop\n",
    "        for step in range(n_steps_max):\n",
    "\n",
    "            # step/interact: calcul de la loss, des gradients, récupération de la récompense et du nouvel état/obs\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "            # step_obs, step_reward, step_done, step_truncated, step_grads, step_loss =\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "            # update (append) current rewards and grads\n",
    "            steps_reward.append(step_reward)\n",
    "            steps_grads.append(step_grads)\n",
    "            steps_loss.append(step_loss)\n",
    "\n",
    "            # continue episode or not\n",
    "            if step_done or step_truncated:\n",
    "                break\n",
    "\n",
    "\n",
    "        # update rewards and grads histories (append)\n",
    "        episodes_reward.append(steps_reward)\n",
    "        episodes_grads.append(steps_grads)\n",
    "        episodes_loss.append(steps_loss)\n",
    "\n",
    "\n",
    "    # outputs\n",
    "    return episodes_reward, episodes_grads, episodes_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b579qp9MhyEK"
   },
   "source": [
    "**Test de la fonction `run_episodes`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6vjcFCGfw4W",
    "outputId": "136ae03f-a42e-4c9c-ca1e-473c0f0881cf"
   },
   "outputs": [],
   "source": [
    "# test de la fonction run_episodes\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "step_obs, step_info = env.reset(seed=SEED)\n",
    "\n",
    "loss_fcn = tf.keras.losses.binary_crossentropy\n",
    "n_episodes = 3\n",
    "n_steps_max = 200\n",
    "\n",
    "# test de la fonction run_episodes\n",
    "episodes_reward, episodes_grads, episodes_loss = run_episodes(env, n_episodes, n_steps_max, policy_model, loss_fcn)\n",
    "\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "# tester les dimensions ici\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "# episodes_reward: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "# episodes_grads: [<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
    "# array([[-0.00076304, -0.00246756, -0.00267922, -0.        , -0.        ],\n",
    "#        [ 0.00128256,  0.00414762,  0.0045034 ,  0.        ,  0.        ],\n",
    "#        [ 0.00255731,  0.00827002,  0.0089794 ,  0.        ,  0.        ],\n",
    "#        [ 0.0026935 ,  0.00871044,  0.00945761,  0.        ,  0.        ]],\n",
    "#       dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
    "# array([-0.05571165, -0.18016429, -0.19561842,  0.        ,  0.        ],\n",
    "#       dtype=float32)>, <tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
    "# array([[-0.01314596],\n",
    "#        [-0.01815959],\n",
    "#        [-0.00327945],\n",
    "#        [-0.        ],\n",
    "#        [-0.        ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.49526525], dtype=float32)>]\n",
    "# episodes_loss: [<tf.Tensor: shape=(), dtype=float32, numpy=0.6837223>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7493657>, <tf.Tensor: shape=(), dtype=float32, numpy=0.7084794>, <tf.Tensor: shape=(), dtype=float32, numpy=0.8146522>, <tf.Tensor: shape=(), dtype=float32, numpy=0.9289527>, <tf.Tensor: shape=(), dtype=float32, numpy=0.4299208>, <tf.Tensor: shape=(), dtype=float32, numpy=0.93320715>, <tf.Tensor: shape=(), dtype=float32, numpy=0.42543203>, <tf.Tensor: shape=(), dtype=float32, numpy=0.49192423>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5630342>, <tf.Tensor: shape=(), dtype=float32, numpy=0.75082785>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5422986>, <tf.Tensor: shape=(), dtype=float32, numpy=0.61447304>, <tf.Tensor: shape=(), dtype=float32, numpy=0.73271173>, <tf.Tensor: shape=(), dtype=float32, numpy=0.5859304>, <tf.Tensor: shape=(), dtype=float32, numpy=0.74739397>, <tf.Tensor: shape=(), dtype=float32, numpy=0.55719686>, <tf.Tensor: shape=(), dtype=float32, numpy=0.626264>]\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVPBYJ61uCJv"
   },
   "source": [
    "\n",
    "**Question: quelles sont les dimensions des sorties episodes_reward, episodes_grads, episodes_loss? Pourquoi?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jVlfAJqylSf"
   },
   "source": [
    "**`Your answer here`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jr_tEH8_rD4O"
   },
   "source": [
    "### 5.2.4 Calculs des différentes sommes cumulées de récompenses\n",
    "\n",
    "Nous allons maintenant estimer la qualité d'effectuer le choix d'une action en calculant le `retour` (ou discounted return), la somme cumulées des récompenses avec rabais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S-S2dRpfw4W"
   },
   "source": [
    "Les fonctions `run_one_learning_step` et `run_episodes` utilisent le modèle pour jouer plusieurs épisodes, stockent:\n",
    "- les gradients tous les paramètres du réseau de neurones du modèle et pour tous les épisodes et toutes les interactions;\n",
    "- les récompenses pour tous les épisodes et toutes les interactions.\n",
    "\n",
    "Il s'agit maintenant de calculer les couts et les gradients pondérés définis dans les algorithmes PG.\n",
    "\n",
    "**Nous allons examiner trois versions:**\n",
    "- la \"reward-to-go\" où les gradients sont **pondérés par une somme des récompenses avec un rabais**:\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N \\left[ \\sum\\limits_{t=0}^{T-1} \\left( \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) \\cdot  \\sum\\limits_{t'=t}^{T-1} \\gamma^{t'-t} r(\\mathbf{s}_{i,t'},\\mathbf{a}_{i,t'}) \\right) \\right]\n",
    "\\end{align*}\n",
    "- la \"reward-to-go\" où les gradients sont **pondérés par une somme des récompenses avec un rabais et avec une baseline moyenne**.\n",
    " \\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N \\left[ \\sum\\limits_{t=0}^{T-1} \\left( \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) \\cdot  \\left(\\sum\\limits_{t'=t}^{T-1}  \\gamma^{t'-t} r(\\mathbf{s}_{i,t'},\\mathbf{a}_{i,t'}) - \\bar{R}_N\\right) \\right)\\right]\\\\\n",
    "\\bar{R}_N &= \\frac{1}{N}\\sum\\limits_{i=1}^{N} R(\\tau_i)\n",
    "\\end{align*}\n",
    "\n",
    "- la \"reward-to-go\" où les gradients sont **pondérés par une somme des récompenses avec un rabais et avec une baseline moyenne normalisation par un ecart type**.\n",
    " \\begin{align*}\n",
    "\\nabla_{\\theta}J(\\theta) &\\approx \\frac{1}{N} \\sum\\limits_{i=1}^N \\left[ \\sum\\limits_{t=0}^{T-1} \\left( \\nabla_{\\theta} \\log \\pi_\\theta(\\mathbf{a}_{i,t}|\\mathbf{s}_{i,t}) \\cdot  \\frac{\\left(\\sum\\limits_{t'=t}^{T-1} \\gamma^{t'-t}  r(\\mathbf{s}_{i,t'},\\mathbf{a}_{i,t'})\\right) - \\bar{R}_N }{s_{N,R}}\\right) \\right]\\\\\n",
    "\\bar{R}_N &= \\frac{1}{N}\\sum\\limits_{i=1}^{N} R(\\tau_i)\n",
    " \\;\\;\\;\\;\\; s_{N,R}^2 = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\left( R(\\tau) - \\bar{R}_N \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Les deux dernières versions \"reward-to-go\" servent à réduire la variance de l'estimateur du gradient connu pour être très bruité (1ère version).\n",
    "\n",
    "**L'un des objectifs de la fin de ce TP est de comparer les résultats obtenus par ces variantes.**\n",
    "\n",
    "\n",
    "**Question 5.3: Créez quatre fonctions permettant à partir d'une liste de rewards de sortir les trois types de reward avec rabais:**\n",
    "- **la première `cpt_returns` calculera les récompenses cumulées:**\n",
    " - **entrées: liste des rewards d'un épisode et discount_rate**\n",
    "- **la seconde `cpt_episodes_returns` calculera les récompenses cumulées sur de nombreux épisodes:**\n",
    " - **entrées: liste (episode) de listes (interactions) de rewards et discount_rate**\n",
    "- **la troisième `cpt_episodes_returns_centered` calculera les récompenses cumulées  moins la moyenne sur de nombreux épisodes:**\n",
    " - **entrées: liste (episode) de listes (interactions) de rewards et discount_rate**\n",
    "- **la quatrième `cpt_episodes_returns_normalised` normalisera les récompenses réduites sur de nombreux épisodes:**\n",
    " - **entrées: liste de listes de rewards et discount_rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUSNdRCRfw4W"
   },
   "outputs": [],
   "source": [
    "def cpt_returns(rewards, discount_rate):\n",
    "\n",
    "    # on fera cela en numpy\n",
    "    discounted_cumrewards = np.array(rewards)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    return discounted_cumrewards\n",
    "\n",
    "def cpt_episodes_returns(episode_rewards, discount_rate):\n",
    "\n",
    "    # calcul des récompenses avec rabais pour tous les épisodes et toutes les interactions\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "\n",
    "    return episodes_returns\n",
    "\n",
    "\n",
    "def cpt_episodes_returns_centered(episode_rewards, discount_rate):\n",
    "\n",
    "    # calcul des récompenses avec rabais pour tous les épisodes et toutes les interactions\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # episode_discounted_rewards = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "    # estimer la moyenne et l'écart type et normaliser les récompenses avec rabais\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    #     episode_discounted_cumrewards_cent=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    return episode_discounted_cumrewards_cent\n",
    "\n",
    "def cpt_episodes_returns_normalised(episode_rewards, discount_rate):\n",
    "\n",
    "    # calcul des récompenses avec rabais pour tous les épisodes et toutes les interactions\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # episode_discounted_rewards = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # estimer la moyenne et l'écart type et normaliser les récompenses avec rabais\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    #     episode_discounted_cumrewards_norm=...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    return episode_discounted_rewards_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elyU1wvIfw4X",
    "outputId": "043c7519-cf54-4137-ec57-d98943e2ebe3"
   },
   "outputs": [],
   "source": [
    "# Test de la fonction\n",
    "arr = cpt_returns([10, 0, -50], discount_rate=0.8)\n",
    "\n",
    "# check\n",
    "print(arr)\n",
    "print('cpt_rewards ok?: ',(arr == np.array([-22, -40, -50])).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dbrn9dXTfw4Y",
    "outputId": "f251b0d9-e82f-4bd1-82bf-abf323984b43"
   },
   "outputs": [],
   "source": [
    "#Test de la fonction (on doit obtenir [array([-22., -40., -50.]), array([26., 20.])])\n",
    "episode_rewards = cpt_episodes_returns([[10., 0., -50.], [10., 20.]], discount_rate=0.8)\n",
    "\n",
    "# check\n",
    "good_episode_rewards =[np.array([-22., -40., -50.]), np.array([26., 20.])]\n",
    "print('cpt_episode_rewards ok?: ', [np.allclose(g,a) for g,a in zip(good_episode_rewards,episode_rewards)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZdf2hRnfw4Y",
    "outputId": "3a7622a2-f8eb-4c86-fd0e-a8ef256af0ab"
   },
   "outputs": [],
   "source": [
    "#Test de la fonction (on doit obtenir [array([ -8.8, -26.8, -36.8]), array([39.2, 33.2])])\n",
    "episode_rewards = cpt_episodes_returns_centered([[10., 0., -50.], [10., 20.]], discount_rate=0.8)\n",
    "\n",
    "# check\n",
    "good_episode_rewards =[np.array([-8.8, -26.8, -36.8]), np.array([39.2, 33.2])]\n",
    "print('cpt_episode_rewards ok?: ', [np.allclose(g,a) for g,a in zip(good_episode_rewards,episode_rewards)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjWyNqRxfw4Y",
    "outputId": "50894023-72e6-4c17-aa91-0173b653f376"
   },
   "outputs": [],
   "source": [
    "#Test de la fonction (on doit obtenir [array([-0.28435071, -0.86597718, -1.18910299]),array([1.26665318, 1.0727777 ])]\n",
    "episode_rewards = cpt_episodes_returns_normalised([[10., 0., -50.], [10., 20.]], discount_rate=0.8)\n",
    "\n",
    "\n",
    "# check\n",
    "print(episode_rewards)\n",
    "good_episode_rewards =[np.array([-0.28435071, -0.86597718, -1.18910299]), np.array([1.26665318, 1.0727777 ])]\n",
    "print('cpt_episode_rewards ok?: ', [np.allclose(g,a) for g,a in zip(good_episode_rewards,episode_rewards)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxEu-S8ifw4Z"
   },
   "source": [
    "## 6. Algorithme final d'apprentissage REINFORCE de type Policy Gradients\n",
    "\n",
    "Finalement, on a tous les blocs pour l'algorithme final que vous compléterez (prenez les bouts de code plus haut si besoin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhtlDsczi0lR"
   },
   "source": [
    "##6.1. Choix des hyper-paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptjmonNEfw4a"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# parametres\n",
    "# ----------------------------------->\n",
    "N_ITERATIONS             = 150\n",
    "N_EPISODES_PER_ITERATION = 10 #     N_EPISODE = 600 # changer par N_EPISODES\n",
    "N_STEPS_MAX_PER_EPISODE  = 200\n",
    "DISCOUNT_RATE            = 0.95\n",
    "SEED                     = 33\n",
    "LEARNING_RATE            = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAZM_f5xi8P7"
   },
   "source": [
    "## 6.2. Définir le modèle de policy $\\pi_\\theta(\\mathbf{a}_{t}|\\mathbf{s}_{t})$\n",
    "\n",
    "**Question 6.1: Remplir le code ci-dessous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjGAB-6Vfw4a"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# Definition of the policy neural network\n",
    "# ----------------------------------->\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "#policy_model = ...\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrPxj6ZajDon"
   },
   "source": [
    "## 6.3. Choisir la tâche à apprendre et comment la résoudre\n",
    "\n",
    "**Question 6.2: Remplir le code ci-dessous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seHGx-zqfw4a"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# loss fonction: choix de la tache à effectuer\n",
    "# ----------------------------------->\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# loss_fcn =...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n",
    "# ----------------------------------->\n",
    "#  choix de l'optimiseur\n",
    "# ----------------------------------->\n",
    "optimizer_fcn = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFDtvjqFjRyC"
   },
   "source": [
    "## 6.4. Création de l'environnement de simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXzxb6EBja0B"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# Création de l'environnement de simulation\n",
    "# ----------------------------------->\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# initialisation des graines aléatoires\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyZjw7JGjk2x"
   },
   "source": [
    "## 6.5. Choix de la fonction de récompenses\n",
    "\n",
    "**Question 6.3: Remplir le code ci-dessous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWSyPCEpjmZz"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# rewards_fcn = ...\n",
    "\n",
    "# ------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpI6ZbxpjJao"
   },
   "source": [
    "## 6.6. Boucle d'*apprentissage*\n",
    "\n",
    "**Question 6.4: Remplir le code ci-dessous**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTbN-_B2fw4b",
    "outputId": "0f72b02a-555d-4b15-a7dd-8f5db4239f97"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------->\n",
    "# gradient descent loop\n",
    "# ----------------------------------->\n",
    "iterations_rewards = []\n",
    "iterations_losses = []\n",
    "\n",
    "for iteration in range(N_ITERATIONS):\n",
    "\n",
    "    # genere des données pour de multiples episodes avec un algorithe \"on-policy\"\n",
    "\n",
    "    # ----------- Your code here --------------------->\n",
    "\n",
    "    # episodes_rewards,...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # on met les récompenses de côté\n",
    "    iterations_rewards.append(episodes_rewards)\n",
    "    iterations_losses.append(episodes_losses)\n",
    "\n",
    "    # calcule de la récompense totale\n",
    "    rewards_total = sum(map(sum, episodes_rewards))\n",
    "\n",
    "    # calcul des récompenses avec rabais\n",
    "    # vous testerez ici le résultat obtenu avec les différentes fonctions codées dans la section précédente\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # episodes_rewards = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "    # print(episodes_returns)\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(iteration, rewards_total / N_EPISODES_PER_ITERATION, end=\"\"))\n",
    "    # print(\"\\rIteration: {}, mean rewards: {:.1f}, mean returns: {:.1f}\".format(iteration, rewards_total / N_EPISODES_PER_ITERATION, sum(episodes_returns)/ N_EPISODES_PER_ITERATION, end=\"\"))\n",
    "\n",
    "    # compute gradients weighted by discounted reward-to-go\n",
    "    episodes_pg_grads = []\n",
    "\n",
    "    # ----------- Your code here --------------------->\n",
    "    # for...: # pour chaque parametre entrainable du modele policy_model.trainable_variable\n",
    "    #     pg_grads = [] # liste des gradients de la fonction de coût PG (grads * reward-to-go)\n",
    "    #     for...: # pour chaque episode (episodes_returns)\n",
    "    #         for...: # pour chaque interaction\n",
    "\n",
    "    # ------------------------------------------------>\n",
    "\n",
    "  # pour chaque parametre entrainable du modele policy_model.trainable_variable\n",
    "    for var_index in range(len(policy_model.trainable_variables)):\n",
    "\n",
    "        # liste des gradients de la fonction de coût PG (grads * reward-to-go)\n",
    "        pg_grads = []\n",
    "\n",
    "        # pour chaque episode (episodes_returns)\n",
    "        for episode_index, episode_returns in enumerate(episodes_returns):\n",
    "\n",
    "            episode_grads = episodes_grads[episode_index]\n",
    "\n",
    "            # chaque interaction\n",
    "            for step, step_returns in enumerate(episode_returns):\n",
    "\n",
    "                step_grads = episode_grads[step][var_index]\n",
    "\n",
    "                # compute le produit des gradients de cross-entropy * return\n",
    "                pg_grads.append(step_returns * step_grads)\n",
    "\n",
    "        # moyenne des gradients\n",
    "        # print(pg_grads)\n",
    "        pg_grads = tf.reduce_mean(pg_grads, axis=0)\n",
    "\n",
    "        # mise à jour de la liste des  gradients\n",
    "        episodes_pg_grads.append(pg_grads)\n",
    "\n",
    "\n",
    "    # apply gradients to policy\n",
    "    optimizer_fcn.apply_gradients(zip(episodes_pg_grads, policy_model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p71oTOPEfw4b"
   },
   "source": [
    "**Question 6.4**: Testez les trois différentes fonctions *reward-to-go* et comparez leurs performances. Tracez en particulier les récompenses moyennes en fonction de l'itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rSffLRyfw4b"
   },
   "source": [
    "**Réponse ici**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "cBQb5Gq4mHY5",
    "outputId": "984a0e94-d1f3-4b73-cf5d-9647c7d2c170"
   },
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "rewards_total = np.array([sum(map(sum, iteration_rewards)) for iteration_rewards in iterations_rewards])\n",
    "losses_total = np.array([sum(map(sum, iteration_losses)) for iteration_losses in iterations_losses])\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(N_ITERATIONS), rewards_total/N_EPISODES_PER_ITERATION)\n",
    "plt.xlabel('Iterations []')\n",
    "plt.ylabel('Loss []')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(N_ITERATIONS), losses_total/N_EPISODES_PER_ITERATION)\n",
    "plt.xlabel('Iterations []')\n",
    "plt.ylabel('Rewards []')\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuILI0w3kGRL"
   },
   "source": [
    "## 6.5. Test de la policy apprise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMoAPN_JxfKt"
   },
   "source": [
    "**Question 6.5: complétez enfin ce code permettant de tester la policy apprise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7cvEIRKnCye"
   },
   "outputs": [],
   "source": [
    "# parametres\n",
    "N_STEPS_MAX_PER_EPISODE = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM6CzaoDfw4b"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------->\n",
    "# test de la policy apprise\n",
    "# ----------------------------------->\n",
    "\n",
    "# initialisation de l'environnement\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# on fixe la graine aléatoire\n",
    "# env.seed(SEED)\n",
    "\n",
    "\n",
    "# recupération de la première obs/state\n",
    "obs, info = env.reset(seed=SEED)\n",
    "\n",
    "\n",
    "# boucle temporelle d'interactions\n",
    "steps_rewards = []\n",
    "frames = []\n",
    "for step in range(N_STEPS_MAX_PER_EPISODE):\n",
    "\n",
    "    # recuperation de l'environnement courant\n",
    "    frames.append(env.render())\n",
    "\n",
    "    # select action (ne pas utiliser la méthode .predict mais l'appel au model simple (model(obs[np.newaxis]) pour la mise en batch))\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # left_proba = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # exploration or action strategy (no exploration in testing mode)\n",
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "    # go_right = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "    # interaction: recupération de la récompense et de la nouvelle observation\n",
    "    obs, reward, done, truncated, info = env.step(go_right)\n",
    "\n",
    "    # mise à jour de la liste des rewards\n",
    "    steps_rewards.append(reward)\n",
    "\n",
    "\n",
    "    # continue or not\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "# output\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4A-ExIBnhaH"
   },
   "source": [
    "**Question: Evaluer cette policy**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "E5f2QzaFrFo6",
    "outputId": "f62e9604-07d8-401b-a59f-4dedd35e32cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------- Your code here --------------------->\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(steps_rewards)), np.array(steps_rewards))\n",
    "plt.xlabel(\"step []\", fontsize=14)\n",
    "plt.ylabel(\"Reward []\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUfFXEz2kXFF"
   },
   "source": [
    "**Affichage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "hNdjJJD7fw4b",
    "outputId": "041ab873-7f86-40cb-9ade-78e90e46e345"
   },
   "outputs": [],
   "source": [
    "plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hxEu-S8ifw4Z"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
