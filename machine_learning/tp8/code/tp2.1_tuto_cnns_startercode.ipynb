{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Deep Learning: Supervised Seafloor classification with CNN** </center> \n",
    "## <center> Machine Learning Programming Exercise 8 part 1: Initiation au Convolutional Neural Networks </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font> |\n",
    "|:-------------: |:----------- |:------ |\n",
    "| binôme 1 | <span style=\"color:red\">DUBEE</span> | <span style=\"color:red\">Melvin</span> |\n",
    "| binôme 2 | <span style=\"color:red\">ROUDAUT</span> | <span style=\"color:red\">Tanguy</span> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "## 0.1 Colab or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True) \n",
    "\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  ipynb_name = 'tp2.1_tuto_cnns_solution.ipynb'\n",
    "\n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# machine learning packages\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "        \n",
    "# options to plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a Convolution? et comment filtrer une image avec tensorflow?\n",
    "\n",
    "Chargez des images au hasard avec sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque filtre est caractérisé:\n",
    "- un support: une longueur et une largeur;\n",
    "- un déplacement (stride);\n",
    "- un nombre de canaux.\n",
    "\n",
    "La dimension de l'image filtrée en sortie est conditionnée par la méthode de padding (voir [ici](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de deux filtres de support (7,7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher ces filtres comme une image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(filters[:, :, 0, 0], cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(filters[:, :, 0, 1], cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec tensorflow, voila comment filtrer par convolution (déplacement de la fenetre de 1 pixel avec un padding permettant d'obtenir la même taille d'images en sortie) des images par un ensemble de filtres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'affichage du filtrage de la première image par le second filtre est alors possible avec la commande suivante. Il est à noter que cette sortie dans le cas d'un CNN est appelé `feature map` ou `carte de descripteurs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1):\n",
    "        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n",
    "        plt.imshow(outputs[image_index, :, :, feature_map_index], cmap=\"gray\", interpolation=\"nearest\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut zoomer sur les sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(images):\n",
    "    return images[150:220, 130:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(crop(images[0, :, :, 0]), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n",
    "    plt.imshow(crop(outputs[0, :, :, feature_map_index]), cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Reprenez la partie précédente en remplacant les filtres par les filtres définis dans le slide intitulé \"Example of convolution filter example applied to image\" de mon cours sur les CNN. Montrez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# Example of convolution filter example applied to image.\n",
    "# filters[, , , 0] = ... # Edge Detection 1\n",
    "# filters[, , , 1] = ... # Edge Detection 2\n",
    "# filters[, , , 2] = ... # Edge Detection 3\n",
    "# filters[, , , 3] = ... # Sharpen\n",
    "# filters[, , , 4] = ... # Box Blur\n",
    "# filters[, , , 5] = ... # Gaussian Blur\n",
    "#...\n",
    "\n",
    "#------------------------------------------------>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Couches caractéristiques d'un CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convolutional Layer / couche de convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier élément d'un réseau de convolution est la couche de convolution (`keras.layers.Conv2D()` [tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)) permettant:\n",
    "- de créer rapidement une banque de filtres (avec leurs caractéristiques) qui seront appris lors de l'apprentissage\n",
    "- initialisés aléatoirement (par défaut `kernel_initializer`)\n",
    "- d'appliquer une non-linéarité après chaque filtre (`activation`)\n",
    "\n",
    "La sortie est alors un tenseur de dimension au moins 4 donné par `activation(conv2d(inputs, kernel) + bias)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: créer une couche de convolution 2D:\n",
    "- 32 filtres de taille (3,3)\n",
    "- le déplacement du filtre sera de 1 pixel\n",
    "- la taille de la sortie sera identique à la taille de l'image d'entrée\n",
    "- l'initialisation des filtres sera faite selon la méthode 'glorot_uniform'\n",
    "- la non-linéarité sera de type 'relu'** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# conv = ...\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il suffit alors de filtrer les images par la commande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv(images)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Afficher la feature map du 16ème filtre pour la seconde image\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Pooling layer / Couche de mise en commun\n",
    "La seconde couche caractéristique est la couche de pooling qui permet:\n",
    "- de diminuer la résolution spatiale\n",
    "- de diminuer le bruit dans l'information et d'augmenter sa pertinence\n",
    "- de produire une décomposition hierarchique de l'information\n",
    "\n",
    "Deux types de pooling sont en général utilisés (soit l'un soit l'autre):\n",
    "- max pooling\n",
    "- average pooling\n",
    "\n",
    "Le seul paramètre à fixer est la taille du support de mise en commun (`pool_size`).\n",
    "Aucun paramétre n'est `apprenable` lors de la phase d'apprentissage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Couche de Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = np.array([crop(image) for image in images], dtype=np.float32)\n",
    "output = max_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(cropped_images[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Output\", fontsize=14)\n",
    "ax2.imshow(output[0])  # plot the output for the 1st image\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Couche de Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pool = keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_avg = avg_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(cropped_images[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Output\", fontsize=14)\n",
    "ax2.imshow(output_avg[0])  # plot the output for the 1st image\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Définir, utiliser et apprendre un CNN avec la base d'images Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Import de la base d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture et séparation du dataset en trois ensembles\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Normalisation de la base d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation (notez bien que la moyenne et std sont estimés sur le training set)\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# on rajoute une dimension channel pour s'adapter au formalisme de tensorflow\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Création de l'architecture du modèle/réseau de neurones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: Créer une architecture basée sur:\n",
    "- un extracteur de descripteurs (Features) composé de:\n",
    "    - une couche de 64 filtres de taille (7,7) avec des activations de type relu, un padding same et acceptant en entrée une image de taille (28,28) en niveau de gris\n",
    "    - une couche de max pooling de support (2,2)\n",
    "    - une couche de 128 filtres de taille (3,3) avec des activations de type relu, un padding same \n",
    "    - une couche de 128 filtres de taille (3,3) avec des activations de type relu, un padding same \n",
    "    - une couche de max pooling de support (2,2)\n",
    "    - une couche de 256 filtres de taille (3,3) avec des activations de type relu, un padding same \n",
    "    - une couche de 256 filtres de taille (3,3) avec des activations de type relu, un padding same \n",
    "    - une couche de max pooling de support (2,2)\n",
    "- une couche permettant la vectorisation des feature map (Flattenisation) et faisant le lien entre extracteur de descripteurs et classifieur\n",
    "- un classifieur composé:\n",
    "    - une couche Fully connected ou Dense de 128 neurones avec des activations de type relu\n",
    "    - une couche Dropout (paramètre 0.5) pour lutter contre l'overfitting \n",
    "    - une couche Fully connected ou Dense de 64 neurones avec des activations de type relu\n",
    "    - une couche Dropout (paramètre 0.5) pour lutter contre l'overfitting \n",
    "    - une couche Fully connected ou Dense finale d'un nombre de neurones = au nombre de classes et des activations de type `softmax` \n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "# model = ...\n",
    "\n",
    "# ------------------------------------------------>\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "- Quel est le nombre de paramètres de ce modèle?\n",
    "- Quel est le nombre de paramètres apprenables de ce modèle?\n",
    "- Quel est le nombre de paramètres apprenables de la dernière couche? Expliquez.\n",
    "- Quel est le nombre de paramètres apprenables de la première couche? Expliquez.\n",
    "- Pourquoi l'activation finale est de type softmax?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Apprentissage du modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1. Choix de la tâche à effectuer = choix de la fonction de loss/pertes/coûts à minimiser\n",
    "see [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = \"sparse_categorical_crossentropy\"  # par alias\n",
    "loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy()# sans alias\n",
    "# attention le sparse fait référence au fait qu'on entre les labels sous la forme d'index et non encodés en OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2. Comment résoudre la tâche de minimisation (choix de l'algorithme d'optimisation)\n",
    "see [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choix de l'optimiseur (On verra cela plus en détail lors du cours sur l'apprentissage)\n",
    "optimizer_fcn = \"nadam\" # par alias \n",
    "optimizer_fcn = tf.keras.optimizers.experimental.Nadam() # sans alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3. Comment évaluer les performances (métriques de performance) lors de l'apprentissage\n",
    "see [here](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_fcn = [\"accuracy\"] # par alias\n",
    "#metrics_fcn = [tf.keras.metrics.SparseCategoricalAccuracy()] # sans alias\n",
    "metrics_fcn = tf.keras.metrics.SparseCategoricalAccuracy() # sans alias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4. Apprendre avec Keras\n",
    "see [here](https://www.tensorflow.org/guide/keras/train_and_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilation ...\n",
    "model.compile(loss=loss_fcn, optimizer=optimizer_fcn, metrics=metrics_fcn)\n",
    "\n",
    "#... et entrainement\n",
    "NB_EPOCHS = 10\n",
    "history = model.fit(X_train, y_train, epochs=NB_EPOCHS, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.5. Prédire avec Keras et le modèle appris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:10] \n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.6. Evaluer les performances du modèle appris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utiliser un modèle CNN pré-entrainé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Construction du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras intègre la possibilité de construire des modèles `classiques` de réseau de neurones de convolution et de télécharger les paramètres/poids déjà entrainés sur des bases de données `classiques`. Les différentes possibilités sont listées [ici](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "Par exemple, voila le code pour construire et télécharger le modèle ResNet50 dont les paramètres préentrainés sur la base d'images `classique` [ImageNet][https://www.image-net.org/]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Prétraitements des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Taille des images\n",
    "Le modèle accepte n'importe quelle taille d'image (c'est un réseau de convolution!) mais comme on fait une décomposition hiérarchique (présence de pooling), la taille minimum des images doit être de [224,224] pour resnet50. Par ailleurs pour l'apprentissage, notez qu'il faut des images de taille fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = tf.image.resize(images, [224, 224])\n",
    "plt.imshow(images_resized[0], interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les modèles pré-entraînés sont toujours entrainés dans des conditions qu'il faut reproduire si on veut les utiliser de manière optimale. On doit par exemple réaliser exactement les mêmes prétraitements. Dans certains cas, les modèles peuvent s'attendre à ce que les entrées soient mises à l'échelle de 0 à 1, ou de -1 à 1, etc. \n",
    "\n",
    "Chaque modèle de `Keras` fournit une fonction `preprocess_input()` que vous pouvez utiliser pour prétraiter vos images. **Ces fonctions partent du principe que les valeurs des pixels sont comprises entre 0 et 255.** Nous devons donc les multiplier par 255 (puisque nous les avons précédemment mises à l'échelle dans la plage 0-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prédiction des images par le modèle\n",
    "\n",
    "Nous pouvons maintenant utiliser le modèle pré-entraîné pour faire des prédictions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = model.predict(inputs)\n",
    "print(Y_proba.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe également une fonction permettant de fournir des infos sur les prédictions. Voir [ici](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/decode_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(\"Image #{}\".format(image_index))\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les classes correctes (monastère et marguerite) apparaissent dans les trois premiers résultats pour les deux images. C'est plutôt bon si l'on considère que le modèle a dû choisir parmi 1000 classes.\n",
    "\n",
    "Comme vous pouvez le constater, il est très facile de créer un bon classifieur d'images à l'aide d'un modèle pré-entraîné. D'autres modèles sont disponibles dans keras.applications , notamment plusieurs variantes de ResNet, des variantes de GoogLeNet comme InceptionV3 et Xception, des variantes de VGGNet, MobileNet et MobileNetV2 (des modèles légers pour les applications applications mobiles), et bien d'autres encore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apprendre un modèle par Transfer Learning\n",
    "\n",
    "Que faire si vous voulez utiliser un classificateur d'images pour vos images qui ne font pas partie d'ImageNet ? Dans ce cas, vous pouvez tout de même bénéficier des modèles pré-entraînés pour réaliser l'apprentissage par transfert.\n",
    "\n",
    "Dans cette partie, on va créer et apprendre un modèle pour classer les photos de fleurs, en réutilisant un modèle Xception pré-entrainé. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, chargeons le jeu de données en utilisant TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation en 3 ensembles de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "index = 0\n",
    "for image, label in train_set_raw.take(9):\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prétraitement des données\n",
    "\n",
    "#### 5.2.1 Prétraitements classiques\n",
    "\n",
    "On a déjà vu comment faire cela. On définit une fonction `preprocess`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Augmentation de la taille de la base d'images (Data Augmentation)\n",
    "\n",
    "Quand on a une base d'images de taille réduite, une technique ultraclassique est de réaliser des tranformations des images qu'on donne en entrée du réseau au moment de l'apprentissage. Ces transformations sont réalisés aléatoirement lors de l'apprentissage. \n",
    "\n",
    "Ce procédé permet d'augmenter virtuellement la taille de la base d'apprentissage et d'indiquer (virtuellement aussi) que ces transformations ne sont pas à considérer pour prédire le label. \n",
    "Il est évident que le choix des transformations est essentiel et doit représenter la physique du phénomène. Dans le domaine optique, on va rencontrer fréquemment des rotations, flip, du croping, des décalages d'image, etc.\n",
    "\n",
    "Un exemple possible est celui-là:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "# \n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "\n",
    "# Nouvelle fonction preprocess\n",
    "def preprocess(image, label, randomize=False):\n",
    "    \n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "        \n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    \n",
    "    return final_image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons maintenant des iterateurs comme lors du dernier TP.\n",
    "On va:\n",
    "- réaliser un mélange de la base d'apprentissage\n",
    "- appliquer preprocess à tous les exemples\n",
    "- mettre en batch de taille 32\n",
    "\n",
    "Voila le code correspondant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "batch_size = 32\n",
    "\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True))\n",
    "train_set = train_set.batch(batch_size)\n",
    "\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons ce qu'il y a dans le train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Modification du modèle pour le fine tuning\n",
    "\n",
    "#### 5.3.1 Chargement du modèle\n",
    "Chargeons un modèle Xception, pré-entraîné sur ImageNet. Nous excluons le classifieur (i.e. le haut du réseau en définissant `include_top=False` ). Ceci exclut la couche de mise en commun de la moyenne globale et la couche de sortie dense (le classifieur). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement du feature extractor (sans classifieur include_top=False)\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Modification de l'architecture\n",
    "Nous ajoutons ensuite notre propre couche de mise en commun de la moyenne globale, basée sur la sortie du modèle de base, suivie d'une couche de sortie dense avec une unité par classe, en utilisant la fonction d'activation softmax. Enfin, nous créons le modèle Keras :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on rajoute séquentiellement des couches de classification\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "\n",
    "# on a recréé le modèle en liant l'entrée de base_model à la sortie \n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the model architecture (ou model.summary)\n",
    "model.summary()\n",
    "# for index, layer in enumerate(base_model.layers):\n",
    "#     print(index, layer.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Gel des paramètres\n",
    "\n",
    "Pour faire du fine tuning, on considère que le modèle pré-entrainé possède un extracteur de features efficace. Il s'agit alors de geler les paramètres liés à cet extracteur de features (au moins au début de l'apprentissage).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principe du fine-tuning = on fixe le feature extractor pendant l'apprentissage\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# paramètres\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "#  Choix de la tâche à effectuer = choix de la fonction de loss/pertes/coûts à minimise\n",
    "loss_fcn = \"sparse_categorical_crossentropy\"  # par alias\n",
    "loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy()# sans alias\n",
    "\n",
    "# choix de l'optimiseur (Comment résoudre la tâche de minimisation )\n",
    "optimizer_fcn = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "\n",
    "# Comment évaluer les performances lors de l'apprentissage\n",
    "#metrics_fcn = [\"accuracy\"] # par alias\n",
    "metrics_fcn = [tf.keras.metrics.SparseCategoricalAccuracy()] # sans alias\n",
    "#metrics_fcn = tf.keras.metrics.SparseCategoricalAccuracy() # sans alias\n",
    "\n",
    "# compilation ...\n",
    "model.compile(loss=loss_fcn, optimizer=optimizer_fcn, metrics=metrics_fcn)\n",
    "\n",
    "#... et entrainement\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=NB_EPOCHS)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir entraîné le modèle pendant quelques époques, sa précision de validation devrait atteindre environ 75-80%, et ne plus faire de progrès. Cela signifie que les couches supérieures sont maintenant assez bien entraînées, nous sommes donc prêts à dégeler toutes les couches (ou vous pouvez essayer de dégeler seulement les couches supérieures), et continuer l'entraînement (n'oubliez pas de compiler le modèle lorsque vous gelez ou dégeler des couches). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On dégèle les couches\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# paramètres\n",
    "NB_EPOCHS = 40\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 0.001\n",
    "\n",
    "#  Choix de la tâche à effectuer = choix de la fonction de loss/pertes/coûts à minimise\n",
    "loss_fcn = \"sparse_categorical_crossentropy\"  # par alias\n",
    "loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy()# sans alias\n",
    "\n",
    "# choix de l'optimiseur (Comment résoudre la tâche de minimisation )\n",
    "optimizer_fcn = keras.optimizers.SGD(learning_rate=LEARNING_RATE, \n",
    "                                     momentum=MOMENTUM, decay=DECAY, nesterov=True)\n",
    "\n",
    "# Comment évaluer les performances lors de l'apprentissage\n",
    "metrics_fcn = [\"accuracy\"] # par alias\n",
    "# metrics_fcn = tf.keras.metrics.Accuracy() # sans alias\n",
    "\n",
    "# compilation ...\n",
    "model.compile(loss=loss_fcn, optimizer=optimizer_fcn, metrics=metrics_fcn)\n",
    "\n",
    "#... et entrainement\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=NB_EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela prendra un certain temps, mais ce modèle devrait atteindre une précision d'environ 95 % sur l'ensemble de test.\n",
    "\n",
    "\n",
    "Vous pouvez passer au tp2.2 maintenant, dans lequel c'est à vous de jouer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
