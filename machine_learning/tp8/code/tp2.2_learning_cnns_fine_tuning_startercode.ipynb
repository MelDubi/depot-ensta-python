{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ce65ea6",
   "metadata": {},
   "source": [
    "# <center> **Deep Learning: Supervised Seafloor classification with CNN** </center> \n",
    "## <center> Machine Learning Programming Exercise 8 part 2.1: **Fine tuning**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bf6c8",
   "metadata": {},
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font> |\n",
    "|:-------------: |:----------- |:------ |\n",
    "| binôme 1 | <span style=\"color:red\">DUBEE</span> | <span style=\"color:red\">Melvin</span> |\n",
    "| binôme 2 | <span style=\"color:red\">ROUDAUT</span> | <span style=\"color:red\">Tanguy</span> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ba027",
   "metadata": {},
   "source": [
    "# 0 - Introduction\n",
    "\n",
    "Disposant d'un ensemble d'images dont on veut prédire la classe, deux possibilités s'offrent pour apprendre un modèle profond de classement basé sur une architecture CNN.\n",
    "\n",
    "- La première possibilité se nomme \"Transfer Learning\" associé au \"fine tuning\" dont les principes sont d'utiliser un réseau de neurones profond entrainé dans un autre contexte et de l'adapter à nos données.\n",
    "    \n",
    "- La seconde possibilité est de créer et d'entrainer un modèle profond ex-nihilo (from scratch, en partant de zéro).\n",
    "\n",
    "L'objectif de ce TP est d'appliquer ces deux possibilités au problème de classification de patchs d'images sonar en types de fond marin que vous avez déjà traités dans les TPs précédents. \n",
    "\n",
    "Nous allons reprendre les fonctions d'import des patchs que vous avez déjà utilisées lors des tps précédents.\n",
    "\n",
    "La création d'ensemble d'apprentissage, de validation et de test se fera en divisant la base en trois parts. Il faudrait pour ce petit jeu de données réaliser une procédure de cross-validation. Compte tenu du temps pour réaliser cette procédure, elle sera ici laissée de coté. \n",
    "\n",
    "\n",
    "### 0.1 - Tutorials CNN et transfer learning par fine tuning\n",
    "\n",
    "Dans un premier temps, le notebook tp2.1 détaille le fonctionnement et la mise en oeuvre des CNNs (en particulier les différentes couches d'un CNN avec des exemples); pour ensuite détailler la procédure liée au fine tuning avec data augmentation à partir de données de type TensorFlow Dataset. \n",
    "\n",
    "Veillez à bien suivre les différentes étapes et à bien comprendre les différentes commandes employées. Si vous voulez d'autres exemples, d'autres ressources supplémentaires sont disponibles:\n",
    "\n",
    "- Les concepts du transfer learning sont expliqués dans les liens ci-dessous:\n",
    "  - https://www.youtube.com/watch?v=FQM13HkEfBk&index=20&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF\n",
    "  - http://cs231n.github.io/transfer-learning/\n",
    "  - https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html et https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html\n",
    "- Des exemples supplémentaires d'implémentation\n",
    "  - https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/CIFAR10_CNN_Classifier.ipynb et https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/CIFAR10_VGG16_Transfer_Learning_Classifier.ipynb\n",
    "  - https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/Dog_Breed_EDA.ipynb et https://github.com/dipanjanS/hands-on-transfer-learning-with-python/blob/master/notebooks/Ch06%20-%20Image%20Recognition%20and%20Classification/Dog_Breed_Transfer_Learning_Classifier.ipynb\n",
    "- cours et des vidéos de Stanford University: https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6, https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7)\n",
    " \n",
    "\n",
    "\n",
    "### 0.2 - Transfer learning par fine tuning sur le dataset seafloor\n",
    "\n",
    "- Vous commencerez par le fine tuning en vous inspirant du tuto fourni ci-dessus pour faire du transfer learning du modèle xception (dont les paramètres ont été appris sur la base d'images \"imageNet\") pour l'appliquer aux patchs d'images sonar. Vous procéderez ainsi:\n",
    "  - compléter ce jupyter notebook  en répondant aux questions\n",
    "  - Résumer l'approche du transfer learning/fine tuning\n",
    "  - Décriver l'architecture du modèle utilisé (xception ici)\n",
    "  - Vous précisérez votre choix concernant les paramètres des fonctions appelées en particulier expliquer votre démarche concernant les phases de preprocessing des images, de data augmentation, de classification, etc.\n",
    "  - Enfin, vous évaluerez les performances obtenues.\n",
    "\n",
    "\n",
    "### 0.3 - Proposition de votre propre achitecture  \n",
    "\n",
    "- Vous proposerez ensuite une architecture de réseau profond convolutif et évaluerez ses performances. \n",
    "- compléter le jupyter notebook `tp2.2_learning_cnns_from_scratch` en répondant aux questions\n",
    "- Expliquez votre architecture et en particulier à quoi servent les couches (et leur enchainement) de votre architecture.\n",
    "- Vous comparerez ensuite les performances obtenus (par rapport à celles obtenues à la partie précédente) sur la matrice de confusion et les métriques de performance classiques.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<FONT COLOR=\"#ff0000\">\n",
    "\n",
    "# A rendre pour le **12/12/23**\n",
    "- **Commenter au maximum votre code (pourquoi vous utilisez tel ou tel bout de code) ou apporter des précisions dans votre CR.**\n",
    "- au choix (**N'oublier pas les deux noms en cas de binômes**):\n",
    "    - un fichier zip avec *.py et un CR au format pdf\n",
    "    - un fichier .ipynb avec compte-rendu et code\n",
    "\n",
    "</FONT>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492df5cc",
   "metadata": {},
   "source": [
    "# 1. Import useful packages \n",
    "Pour pouvoir commencer, vous importerez les librairies suivantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c26746",
   "metadata": {},
   "source": [
    "## 1.1 Colab or not colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f098e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True) \n",
    "\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  ipynb_name = 'tp2.2_learning_cnns_fine_tuning_startecode.ipynb'\n",
    "\n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2f782",
   "metadata": {},
   "source": [
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90206a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "# machine learning packages\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efaed5",
   "metadata": {},
   "source": [
    "## 2 - Loading and visualizing the dataset \n",
    "\n",
    "### 2.1 Loading\n",
    "\n",
    "Les données sont les mêmes que pour le TP5. La procédure pour importer les données est légèrement différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ff57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# LOAD Data, Features, Labels\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Définition du chemin vers le répertoire dataset pour les images et les labels\n",
    "DATASET_PATH = r'./dataset/imgs/'\n",
    "LABEL_PATH = r'./dataset/labels/labels.csv'\n",
    "\n",
    "# Flag pour le chargement des images\n",
    "# True for fine tuning | False for from scratch\n",
    "flag_load_as_rgb = True \n",
    "\n",
    "# Taille d'entrée du modèle (>=71 pour xception)\n",
    "target_size = 200\n",
    "\n",
    "# Import des données\n",
    "def importData():\n",
    "\n",
    "    # Charger le fichier CSV\n",
    "    dataset_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "    # We add another column to the labels dataset to identify image path\n",
    "    dataset_df['image_path'] = dataset_df.apply(lambda row: (DATASET_PATH + row[\"id\"]), axis=1)\n",
    "\n",
    "    # Récupération des labels\n",
    "    label_names = dataset_df['seafloor']\n",
    "\n",
    "    # Chargement des images et changement de la taille des images et duplication sur canaux RGB\n",
    "    batch_imgs = []\n",
    "    for img in dataset_df['image_path'].values.tolist():\n",
    "        \n",
    "        if flag_load_as_rgb:\n",
    "            tmp = load_img(img, color_mode = \"rgb\", target_size=(target_size, target_size))\n",
    "        else:\n",
    "            tmp = load_img(img, color_mode = \"grayscale\", target_size=(target_size, target_size))\n",
    "        \n",
    "        # Converts a PIL Image instance to a Numpy array\n",
    "        tmp = img_to_array(tmp)\n",
    "        batch_imgs.append(tmp)\n",
    "    \n",
    "    # conversion en numpy array\n",
    "    batch_imgs = np.array(batch_imgs).astype('float32')\n",
    "            \n",
    "    return batch_imgs, label_names\n",
    "\n",
    "# call importData\n",
    "batch_imgs, label_names = importData()\n",
    "\n",
    "# variables utiles\n",
    "instance_nb, height, width, channel_nb = batch_imgs.shape\n",
    "feature_nb = batch_imgs.shape[1]*batch_imgs.shape[2]\n",
    "channel_nb = batch_imgs.shape[-1]\n",
    "\n",
    "print('dimension du batch d''images: {}'.format(batch_imgs.shape))\n",
    "print('dimension des labels: {}'.format(label_names.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fd620",
   "metadata": {},
   "source": [
    "***Question 2.1: Quel est le nombre de canaux de chaque patch? Pourquoi?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaac204",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cea788",
   "metadata": {},
   "source": [
    "### 2.2 - Display patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0188c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a Sample Set as a grid (grid_height x grid_width)\n",
    "grid_width=5\n",
    "grid_height=5\n",
    "\n",
    "# choix aléatoire d'un nombre (grid_width*grid_height) d'images à afficher\n",
    "ind = np.random.randint(0, instance_nb, grid_width*grid_height)\n",
    "\n",
    "# image counter\n",
    "img_idx = 0\n",
    "\n",
    "# display\n",
    "\n",
    "# display a \n",
    "f, ax = plt.subplots(grid_width, grid_height)\n",
    "f.set_size_inches(6, 6)\n",
    "for i in range(0, grid_width):\n",
    "    for j in range(0, grid_height):\n",
    "        ax[i][j].axis('off')\n",
    "        ax[i][j].set_title(label_names[ind[img_idx]])\n",
    "        ax[i][j].imshow(batch_imgs[ind[img_idx],:,:,0], cmap='copper')\n",
    "\n",
    "        # update image counter\n",
    "        img_idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9107a2",
   "metadata": {},
   "source": [
    "## 3. Pré-traitements en vue de la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ad02c",
   "metadata": {},
   "source": [
    "### 3.1 Séparation des données en ensembles\n",
    "\n",
    "Les lignes suivantes de code permettent de créer trois ensembles: apprentissage, validation et test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# PREPARE DATASETS Split into 3 sets\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "def prepare_datasets(batch_imgs, label_names):\n",
    "    print('Split into 3 sets...')\n",
    "\n",
    "    tmp = train_test_split(batch_imgs,\n",
    "                           label_names,\n",
    "                           test_size=0.5,\n",
    "                           stratify=np.array(label_names),\n",
    "                           random_state=42)\n",
    "    batch_imgs_train, batch_imgs_test, labelNames_train, labelNames_test = tmp\n",
    "    \n",
    "    tmp = train_test_split(batch_imgs_test,\n",
    "                           labelNames_test,\n",
    "                           test_size=0.5,\n",
    "                           stratify=np.array(labelNames_test),\n",
    "                           random_state=42)\n",
    "    batch_imgs_test, batch_imgs_val, labelNames_test, labelNames_val = tmp\n",
    "    \n",
    "    # taille du dataset\n",
    "    dataset_size = batch_imgs.shape[0]\n",
    "\n",
    "    # nb de classes\n",
    "    labelNames_unique = label_names.unique()\n",
    "    label_nb = labelNames_unique.shape[0]\n",
    "\n",
    "\n",
    "    return batch_imgs_train, labelNames_train, batch_imgs_val, labelNames_val, batch_imgs_test, labelNames_test\n",
    "\n",
    "# call prepare_datasets\n",
    "batch_imgs_train, labelNames_train, batch_imgs_val, labelNames_val, batch_imgs_test, labelNames_test = prepare_datasets(batch_imgs, label_names)\n",
    "\n",
    "# Vérification des formats des ensembles\n",
    "print(\"Format du set de train : \"     , batch_imgs_train.shape)\n",
    "print(\"Format du set de validation : \", batch_imgs_val.shape)\n",
    "print(\"Format du set de test : \"      , batch_imgs_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ef892",
   "metadata": {},
   "source": [
    "### 3.2 Gestion des labels\n",
    "\n",
    "Les lignes de code suivantes permettent de disposer des labels pour chaque ensemble de données dans différents [codages](https://scikit-learn.org/stable/modules/preprocessing_targets.html) (noms, indices, [one-hot-encoding](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features), etc.). Ces codages serviront dans la suite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "#  transformation des labels selon différents codages\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#  Noms des labels\n",
    "labelNames_unique = label_names.unique()\n",
    "\n",
    "# nb de classes\n",
    "label_nb = labelNames_unique.shape[0]\n",
    "\n",
    "# enc labelNames to indices\n",
    "encName2Ind = preprocessing.LabelEncoder()\n",
    "encName2Ind.fit(labelNames_unique)\n",
    "labelIndices_unique = encName2Ind.transform(labelNames_unique)\n",
    "labelIndices  = encName2Ind.transform(label_names)\n",
    "\n",
    "# enc indices to  one-hot-encoding\n",
    "encInd2Ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "encInd2Ohe.fit(labelIndices_unique.reshape(-1, 1))\n",
    "labelOhe = encInd2Ohe.transform(labelIndices.reshape(-1, 1))\n",
    "\n",
    "# enc labelNames to  one-hot-encoding\n",
    "encName2Ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "encName2Ohe.fit(labelNames_unique.reshape(-1, 1))\n",
    "#labelOhe2 = encName2Ohe.transform(label_names.reset_index(drop=True).values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Conversion des noms des labels en indices\n",
    "labelInd_train = encName2Ind.transform(labelNames_train)\n",
    "labelInd_val = encName2Ind.transform(labelNames_val)\n",
    "labelInd_test = encName2Ind.transform(labelNames_test)\n",
    "\n",
    "# Conversion des noms des labels en  one-hot-encoding\n",
    "labelOhe_train = encName2Ohe.transform(labelNames_train.reset_index(drop=True).values.reshape(-1, 1))\n",
    "labelOhe_val   = encName2Ohe.transform(labelNames_val.reset_index(drop=True).values.reshape(-1, 1))\n",
    "labelOhe_test  = encName2Ohe.transform(labelNames_test.reset_index(drop=True).values.reshape(-1, 1))\n",
    "\n",
    "# autre solution avec panda\n",
    "# labelOhe_train = pd.get_dummies(labelNames_train.reset_index(drop=True)).values\n",
    "# labelOhe_val   = pd.get_dummies(labelNames_val.reset_index(drop=True)).values\n",
    "# labelOhe_test  = pd.get_dummies(labelNames_test.reset_index(drop=True)).values\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# DATASETS Summary\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "#  Noms et indices des labels\n",
    "labelNames_unique   = label_names.unique()\n",
    "labelIndices_unique = encName2Ind.transform(labelNames_unique)\n",
    "label_nb            = labelNames_unique.shape[0]\n",
    "\n",
    "# taille du dataset\n",
    "dataset_size = batch_imgs.shape[0]\n",
    "\n",
    "print('------------------------------')\n",
    "print('Seafloor Training Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Feature Shape:', batch_imgs_train.shape)\n",
    "print('Labels Shape:', labelNames_train.shape)\n",
    "print('labels distrib over labels:', [sum(labelInd_train == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Seafloor Validation Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Validation Features Shape:', batch_imgs_val.shape)\n",
    "print('Validation Labels Shape:', labelNames_val.shape)\n",
    "print('labels distrib over classe:', [sum(labelInd_val == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Seafloor Testing Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Testing Features Shape:', batch_imgs_test.shape)\n",
    "print('Testing Labels Shape:', labelNames_test.shape)\n",
    "print('labels distrib over classe:', [sum(labelInd_test == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Split into 3 sets...done')\n",
    "\n",
    "\n",
    "print('Encoding done...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e714f8",
   "metadata": {},
   "source": [
    "### 3.3 Transformation des ensembles en objet tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db66c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Création du dataset en objet tf.data.Dataset\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((batch_imgs_train, labelInd_train))\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((batch_imgs_val, labelInd_val))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((batch_imgs_test, labelInd_test))\n",
    "\n",
    "# variables utiles\n",
    "train_instance_nb = ds_train.cardinality().numpy()\n",
    "print(train_instance_nb)\n",
    "val_instance_nb = ds_val.cardinality().numpy()\n",
    "print(val_instance_nb)\n",
    "test_instance_nb = ds_test.cardinality().numpy()\n",
    "print(test_instance_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8621871",
   "metadata": {},
   "source": [
    "## 4. Fine tune a pretrained model for seafloor classification\n",
    "\n",
    "### 4.1  XCEPTION model\n",
    "\n",
    "Vous commencerez par le fine tuning du modèle XCEPTION en vous inspirant du tuto fourni (dont les paramètres ont été appris sur la base d'images \"imageNet\") pour l'appliquer aux patchs d'images sonar. Vous procéderez ainsi (attention les réponses à ces questions sont toutes notées):\n",
    "- Suivre les différentes étapes du tuto.\n",
    "- Décrire l'architecture du modèle utilisé (xception ici): nombre de paramètres entrainables et non entrainables, particularités par rapport aux auters réseaux, etc.\n",
    "- Vous précisérez votre choix concernant les paramètres des fonctions appelées. En particulier expliquer votre démarche concernant les phases de preprocessing des images, de data augmentation, de couches de classification, etc.\n",
    "- **remarque**: comme les images sonar sont en niveaux de gris et que le modèle VGG prend en entrée des images couleurs, il s'agira de dupliquer ce canal sur les canaux R, G et B. \n",
    "\n",
    "- Enfin, vous évaluerez proprement les performances obtenues (learning curves, matrice de confusion).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933cb142",
   "metadata": {},
   "source": [
    "#### 4.1.1 Paramètres  (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ec0f0",
   "metadata": {},
   "source": [
    "#### 4.1.2 Data preprocessing (potentiellement resize, normalisation, data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ceb273-c25e-4237-b155-57da60087913",
   "metadata": {},
   "source": [
    "**Question: Expliquez votre code. Quels sont vos choix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a378db-5b17-454b-8cc3-b9181c18f67d",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf985da0",
   "metadata": {},
   "source": [
    "#### 4.1.3 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b541fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960696d",
   "metadata": {},
   "source": [
    "#### 4.1.4 Learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf672af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a01b8",
   "metadata": {},
   "source": [
    "#### 4.1.5 Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ad932",
   "metadata": {},
   "source": [
    "#### 4.1.6 Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "RESULT_PATH = './results/'\n",
    "\n",
    "# description\n",
    "json_finename = RESULT_PATH+'/'+MODEL_TYPE+\"_finetunedmodel.json\"\n",
    "model_finename = RESULT_PATH+'/'+MODEL_TYPE+\"_finetunedmodel.h5\"\n",
    "weights_filename = RESULT_PATH+'/'+MODEL_TYPE+\"_finetunedweights.h5\"\n",
    "hist_filename = RESULT_PATH+'/'+MODEL_TYPE+\"_finetunedhistory.npy\"\n",
    "\n",
    "\n",
    "# save json model\n",
    "model_json = model.to_json()\n",
    "with open(json_finename, \"w\") as json_file:\n",
    "     json_file.write(model_json)\n",
    "\n",
    "# save model and weights\n",
    "model.save(model_finename)\n",
    "\n",
    "# save weights of the model\n",
    "model.save_weights(weights_filename)\n",
    "print(\"Model saved to disk\")\n",
    "\n",
    "# Fit history saving\n",
    "np.save(hist_filename, history.history)\n",
    "\n",
    "# with open(hist_filename, \"w\") as file_pi:\n",
    "#      pickle.dump(history.history, file_pi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5942da5",
   "metadata": {},
   "source": [
    "**Question 4.1: Décrivez l'approche du fine tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf92f7d",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3cc2c",
   "metadata": {},
   "source": [
    "**Question 4.2: Décrire l'architecture du modèle utilisé (xception ici)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123d7b9",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94f8e3",
   "metadata": {},
   "source": [
    "**Question 4.3: précisérez votre choix concernant les paramètres des fonctions appelées. En particulier expliquer votre démarche concernant les phases de preprocessing des images, de data augmentation, de couches de classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0f657",
   "metadata": {},
   "source": [
    "**_`Double cliquez ici pour écrire votre réponse ici!`_**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8dc22",
   "metadata": {},
   "source": [
    "### 4.2 Fine tune the VGG16, inceptionV3 models for seafloor classification\n",
    "\n",
    "- Recommencez la même démarche avec les modèles [VGG16 et inceptionV3](https://keras.io/applications/)\n",
    "- comparez les résultats obtenus par les différents modèles.\n",
    "\n",
    "- (Bonus) Essayez et comparez les résultats obtenus par d'autres architectures (Resnet, Dense etc...[https://keras.io/applications](https://keras.io/applications/).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
