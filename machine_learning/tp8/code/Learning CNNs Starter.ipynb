{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501e38d6",
   "metadata": {},
   "source": [
    "# <center> **Deep Learning: Supervised Seafloor classification with CNN** </center> \n",
    "## <center> Machine Learning Programming Exercise 8 part 2.1: **From scratch**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960313b9",
   "metadata": {},
   "source": [
    "| <font size=6,font color='red'>Monôme / binôme</font> | <font size=6,font color='red'>Nom</font> | <font size=6,font color='red'>Prénom</font> |\n",
    "|:-------------: |:----------- |:------ |\n",
    "| binôme 1 | <span style=\"color:red\">DUBEE</span> | <span style=\"color:red\">Melvin</span> |\n",
    "| binôme 2 | <span style=\"color:red\">ROUDAUT</span> | <span style=\"color:red\">Tanguy</span> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10851ebb",
   "metadata": {},
   "source": [
    "Vous proposerez une architecture de réseau profond convolutif, apprendrez le modèle avec les patchs et évaluerez ses performances. \n",
    "- Expliquez votre architecture et en particulier à quoi servent les couches (et leur enchainement) de votre architecture.\n",
    "- Vous comparerez ensuite les performances obtenus (par rapport à celles obtenues à la partie précédente) sur la matrice de confusion et les métriques de performance classiques.\n",
    "\n",
    "- Suivre les différentes étapes du tuto.\n",
    "- **remarque**: comme les images sonar sont en niveaux de gris, il s'agira de conserver un seul canal.\n",
    "\n",
    "- Enfin, vous évaluerez proprement les performances obtenues (learning curves, matrice de confusion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108081af-8eb9-4572-8cf8-414b41fb0f89",
   "metadata": {},
   "source": [
    "# 1. Import useful packages \n",
    "Pour pouvoir commencer, vous importerez les librairies suivantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604aee9f-1243-4c6e-a239-a084f8fcd266",
   "metadata": {},
   "source": [
    "## 1.1 Colab or not colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdf32a-4b2e-48e2-a3d7-7c8ca08bb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import sys,os,glob\n",
    "\n",
    "# Colab preamble\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "\n",
    "  # mount google drive directories\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive', force_remount=True) \n",
    "\n",
    "\n",
    "  # ----------- Your code here --------------------->\n",
    "  # replace the ipynb_name (below) with the name of your jupyter notebook file\n",
    "\n",
    "  ipynb_name = 'tp2.2_learning_cnns_from_scratch_startecode.ipynb'\n",
    "\n",
    "  # ------------------------------------------------>\n",
    "\n",
    "  ipynb_name = glob.glob(os.getcwd() + '/**/' + ipynb_name, recursive = True)\n",
    "  code_folder = os.path.dirname(ipynb_name[0])\n",
    "\n",
    "  # change to the right folder\n",
    "  %cd \"$code_folder\"\n",
    "  !ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850476e-96ba-436b-b99d-c84949f0db7b",
   "metadata": {},
   "source": [
    "## 1.2 Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d41e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import pickle\n",
    "\n",
    "# machine learning packages\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124ed5d-5873-4e6c-875f-97513c9cd4a0",
   "metadata": {},
   "source": [
    "## 1.3 Useful codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c96a3b-7d14-4b6d-95d9-219a54064360",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Définition du chemin vers le répertoire dataset pour les images et les labels\n",
    "DATASET_PATH = r'./dataset/imgs/'\n",
    "LABEL_PATH = r'./dataset/labels/labels.csv'\n",
    "\n",
    "# Flag pour le chargement des images\n",
    "# True for fine tuning | False for from scratch\n",
    "flag_load_as_rgb = False \n",
    "\n",
    "# Taille d'entrée du modèle (>=71 pour xception)\n",
    "target_size = 200\n",
    "\n",
    "# Import des données\n",
    "def importData():\n",
    "\n",
    "    # Charger le fichier CSV\n",
    "    dataset_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "    # We add another column to the labels dataset to identify image path\n",
    "    dataset_df['image_path'] = dataset_df.apply(lambda row: (DATASET_PATH + row[\"id\"]), axis=1)\n",
    "\n",
    "    # Récupération des labels\n",
    "    label_names = dataset_df['seafloor']\n",
    "\n",
    "    # Chargement des images\n",
    "\n",
    "\n",
    "    # Changement de la taille des images et duplication sur canaux RGB\n",
    "   \n",
    "    batch_imgs = []\n",
    "    for img in dataset_df['image_path'].values.tolist():\n",
    "        \n",
    "        if flag_load_as_rgb:\n",
    "            tmp = load_img(img, color_mode = \"rgb\", target_size=(target_size, target_size))\n",
    "        else:\n",
    "            tmp = load_img(img, color_mode = \"grayscale\", target_size=(target_size, target_size))\n",
    "        \n",
    "        # Converts a PIL Image instance to a Numpy array\n",
    "        tmp = img_to_array(tmp)\n",
    "        batch_imgs.append(tmp)\n",
    "    \n",
    "    # conversion en numpy array\n",
    "    batch_imgs = np.array(batch_imgs).astype('float32')\n",
    "            \n",
    "    return batch_imgs, label_names\n",
    "\n",
    "# call importData\n",
    "batch_imgs, label_names = importData()\n",
    "\n",
    "# variables utiles\n",
    "instance_nb, height, width, channel_nb = batch_imgs.shape\n",
    "feature_nb = batch_imgs.shape[1]*batch_imgs.shape[2]\n",
    "channel_nb = batch_imgs.shape[-1]\n",
    "\n",
    "print('dimension du batch d''images: {}'.format(batch_imgs.shape))\n",
    "print('dimension des labels: {}'.format(label_names.shape))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# PREPARE DATASETS Split into 3 sets\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "def prepare_datasets(batch_imgs, label_names):\n",
    "    print('Split into 3 sets...')\n",
    "\n",
    "    tmp = train_test_split(batch_imgs,\n",
    "                           label_names,\n",
    "                           test_size=0.5,\n",
    "                           stratify=np.array(label_names),\n",
    "                           random_state=42)\n",
    "    batch_imgs_train, batch_imgs_test, labelNames_train, labelNames_test = tmp\n",
    "    \n",
    "    tmp = train_test_split(batch_imgs_test,\n",
    "                           labelNames_test,\n",
    "                           test_size=0.5,\n",
    "                           stratify=np.array(labelNames_test),\n",
    "                           random_state=42)\n",
    "    batch_imgs_test, batch_imgs_val, labelNames_test, labelNames_val = tmp\n",
    "    \n",
    "    # taille du dataset\n",
    "    dataset_size = batch_imgs.shape[0]\n",
    "\n",
    "    # nb de classes\n",
    "    labelNames_unique = label_names.unique()\n",
    "    label_nb = labelNames_unique.shape[0]\n",
    "\n",
    "\n",
    "    return batch_imgs_train, labelNames_train, batch_imgs_val, labelNames_val, batch_imgs_test, labelNames_test\n",
    "\n",
    "# call prepare_datasets\n",
    "batch_imgs_train, labelNames_train, batch_imgs_val, labelNames_val, batch_imgs_test, labelNames_test = prepare_datasets(batch_imgs, label_names)\n",
    "\n",
    "# Vérification des formats des ensembles\n",
    "print(\"Format du set de train : \"     , batch_imgs_train.shape)\n",
    "print(\"Format du set de validation : \", batch_imgs_val.shape)\n",
    "print(\"Format du set de test : \"      , batch_imgs_test.shape)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "#  transformation des labels selon différents codages\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#  Noms des labels\n",
    "labelNames_unique = label_names.unique()\n",
    "\n",
    "# nb de classes\n",
    "label_nb = labelNames_unique.shape[0]\n",
    "\n",
    "# enc labelNames to indices\n",
    "encName2Ind = preprocessing.LabelEncoder()\n",
    "encName2Ind.fit(labelNames_unique)\n",
    "labelIndices_unique = encName2Ind.transform(labelNames_unique)\n",
    "labelIndices  = encName2Ind.transform(label_names)\n",
    "\n",
    "# enc indices to  one-hot-encoding\n",
    "encInd2Ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "encInd2Ohe.fit(labelIndices_unique.reshape(-1, 1))\n",
    "labelOhe = encInd2Ohe.transform(labelIndices.reshape(-1, 1))\n",
    "\n",
    "# enc labelNames to  one-hot-encoding\n",
    "encName2Ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "encName2Ohe.fit(labelNames_unique.reshape(-1, 1))\n",
    "#labelOhe2 = encName2Ohe.transform(label_names.reset_index(drop=True).values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Conversion des noms des labels en indices\n",
    "labelInd_train = encName2Ind.transform(labelNames_train)\n",
    "labelInd_val = encName2Ind.transform(labelNames_val)\n",
    "labelInd_test = encName2Ind.transform(labelNames_test)\n",
    "\n",
    "# Conversion des noms des labels en  one-hot-encoding\n",
    "labelOhe_train = encName2Ohe.transform(labelNames_train.reset_index(drop=True).values.reshape(-1, 1))\n",
    "labelOhe_val   = encName2Ohe.transform(labelNames_val.reset_index(drop=True).values.reshape(-1, 1))\n",
    "labelOhe_test  = encName2Ohe.transform(labelNames_test.reset_index(drop=True).values.reshape(-1, 1))\n",
    "\n",
    "# autre solution avec panda\n",
    "# labelOhe_train = pd.get_dummies(labelNames_train.reset_index(drop=True)).values\n",
    "# labelOhe_val   = pd.get_dummies(labelNames_val.reset_index(drop=True)).values\n",
    "# labelOhe_test  = pd.get_dummies(labelNames_test.reset_index(drop=True)).values\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# DATASETS Summary\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "#  Noms et indices des labels\n",
    "labelNames_unique   = label_names.unique()\n",
    "labelIndices_unique = encName2Ind.transform(labelNames_unique)\n",
    "label_nb            = labelNames_unique.shape[0]\n",
    "\n",
    "# taille du dataset\n",
    "dataset_size = batch_imgs.shape[0]\n",
    "\n",
    "print('------------------------------')\n",
    "print('Seafloor Training Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Feature Shape:', batch_imgs_train.shape)\n",
    "print('Labels Shape:', labelNames_train.shape)\n",
    "print('labels distrib over labels:', [sum(labelInd_train == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Seafloor Validation Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Validation Features Shape:', batch_imgs_val.shape)\n",
    "print('Validation Labels Shape:', labelNames_val.shape)\n",
    "print('labels distrib over classe:', [sum(labelInd_val == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Seafloor Testing Set Summary ')\n",
    "print('------------------------------')\n",
    "print('Testing Features Shape:', batch_imgs_test.shape)\n",
    "print('Testing Labels Shape:', labelNames_test.shape)\n",
    "print('labels distrib over classe:', [sum(labelInd_test == ind) for ind in labelIndices_unique])\n",
    "print('------------------------------')\n",
    "print('Split into 3 sets...done')\n",
    "\n",
    "\n",
    "print('Encoding done...')\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Normalisation \n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# estimation\n",
    "images_mean = batch_imgs_train.mean(axis=0, keepdims=True)\n",
    "images_std = batch_imgs_train.std(axis=0, keepdims=True) + 1e-7\n",
    "\n",
    "# normalisation\n",
    "batch_imgs_train = (batch_imgs_train - images_mean) / images_std\n",
    "batch_imgs_val = (batch_imgs_val - images_mean) / images_std\n",
    "batch_imgs_test = (batch_imgs_test - images_mean) / images_std\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "# Création du dataset en objet tf.data.Dataset\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((batch_imgs_train, labelInd_train))\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((batch_imgs_val, labelInd_val))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((batch_imgs_test, labelInd_test))\n",
    "\n",
    "# variables utiles\n",
    "train_instance_nb = ds_train.cardinality().numpy()\n",
    "val_instance_nb = ds_val.cardinality().numpy()\n",
    "test_instance_nb = ds_test.cardinality().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252bf851",
   "metadata": {},
   "source": [
    "### 5.1 Paramètres (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ce7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8824b",
   "metadata": {},
   "source": [
    "### 5.2 Data preprocessing (normalisation, data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62dd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "# ------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47f8b3",
   "metadata": {},
   "source": [
    "### 5.3 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------>\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd579c6a",
   "metadata": {},
   "source": [
    "### 5.4 Learning the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd67f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Your code here --------------------->\n",
    "\n",
    "#------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba48195",
   "metadata": {},
   "source": [
    "### 5.5 Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86216ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- Your code here --------------------->\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe714213",
   "metadata": {},
   "source": [
    "### 4.6 Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a50a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "# description\n",
    "json_finename = RESULT_PATH+'/'+MODEL_TYPE+\"_fromscratchmodel.json\"\n",
    "model_finename = RESULT_PATH+'/'+MODEL_TYPE+\"_fromscratchmodel.h5\"\n",
    "weights_filename = RESULT_PATH+'/'+MODEL_TYPE+\"_fromscratchweights.h5\"\n",
    "hist_filename = RESULT_PATH+'/'+MODEL_TYPE+\"_fromscratchhistory.npy\"\n",
    "\n",
    "\n",
    "# save yaml model\n",
    "model_json = model.to_json()\n",
    "with open(json_finename, \"w\") as json_file:\n",
    "     json_file.write(model_json)\n",
    "\n",
    "# save model and weights\n",
    "model.save(model_finename)\n",
    "\n",
    "# save weights of the model\n",
    "model.save_weights(weights_filename)\n",
    "print(\"Model saved to disk\")\n",
    "\n",
    "# Fit history saving\n",
    "np.save(hist_filename, history.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d33c97",
   "metadata": {},
   "source": [
    "**Question 5.1: Expliquez votre architecture et en particulier à quoi servent les couches (et leur enchainement) de votre architecture.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3594ef",
   "metadata": {},
   "source": [
    "_Double-cliquez ici pour écrire votre réponse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be8258",
   "metadata": {},
   "source": [
    "**Question 5.2: Faites un bilan des performances obtenus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7f232",
   "metadata": {},
   "source": [
    "_Double-cliquez ici pour écrire votre réponse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba4a3c",
   "metadata": {},
   "source": [
    "## 6. Bilan des modèles appris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9d4dc",
   "metadata": {},
   "source": [
    "**Question 6.1: Comparez les performances obtenues (par rapport à celles obtenues à la partie précédente) sur la matrice de confusion et les métriques de performance classiques. Et donnez les avantages et inconvénients de chaque approche.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab9d69",
   "metadata": {},
   "source": [
    "_Double-cliquez ici pour écrire votre réponse_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c21e33",
   "metadata": {},
   "source": [
    "## 7. Fonctions d'aide éventuelle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction d'aide pour afficher les courbes d'apprentissage\n",
    "\n",
    "def learningCurves(history,title):\n",
    "    #Learning curve plotting\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    t = f.suptitle(title, fontsize=12)\n",
    "    f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "    epochs = list(range(1,NB_EPOCHS+1))\n",
    "    ax1.plot(epochs, history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_xticks(epochs)\n",
    "    ax1.set_ylabel('Accuracy Value')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(epochs, history['loss'], label='Train Loss')\n",
    "    ax2.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_xticks(epochs)\n",
    "    ax2.set_ylabel('Loss Value')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    \n",
    "# fonction callback (à rajouter en option à model.fit) pour suivre l'évolution de la matrice de confusion au long de l'apprentissage\n",
    "# credit: K. Bedin (ROB 2020) et D. Eleye (ROB 2023) développé lors du cours\n",
    "class ConfusionEvaluation(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "        Fonction callback pour la méthode 'fit_generator()' permettant d'afficher \n",
    "        la matrice de confusion à chaque fin d'Epoch.\n",
    "        Cela permet de visualiser concraitement l'évolution de la classification au cours de l'apprentissage.\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data=()):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        preds_Inception = self.model.predict(self.X_val)\n",
    "        matrixInception = confusion_matrix(self.y_val,preds_Inception.argmax(axis=1))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(matrixInception)\n",
    "\n",
    "# cbk_matconf = ConfusionEvaluation(validation_data=(ds_val, labelInd_val))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
