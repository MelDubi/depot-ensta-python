\section{Classificateur pour un réseau de neurone}

Cette section a pour but d'implémenter une partie d'un algorithme de rétropropagation dit 
\textit{Backpropagation}. Cet algorithme permettra d'entrainer notre réseau de neurone afin d'obtenir
un coût minimal. Le principe du \textit{Backpropagation} est d'effectuer de manière itérative un calcul
entre le gradient de la fonction de coût et les poids du réseau de neurone. Ainsi, à chaque itération, les
poids du réseau sont ajustés pour avoir une prédiction plus précise. 

\subsection{Fonction de coût}

Tout d'abord, comme pour les TP passés, nous allons effectuer le calcul du coût $J(\theta)$ qui
permet de mesurer la qualité de la prédiction, si le coût est faible alors notre prédiction est proche 
des valeurs réelles et inversement si le coût est important. La formule utilisée 
pour calculer le coût change puisque l'on travail sur un réseau de neurone. 

\begin{equation}
    J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[ -y_k^{(i)} \log((h_\theta(x^{(i)}))_{k}) + (1 - y_k^{(i)}) \log(1 - (h_\theta(x^{(i)}))_{k}) \right]
\end{equation}

Pour cette section, il faut bien faire attention à l'implémentation de la matrice Y. En effet, il faut
veiller à bien recoder les étiquettes en tant que vecteur composé de 0 et de 1. Pour mieux visualiser,
voici la matrice \ref{matrice:y} correspondant à $y^{(i)}$ si $x^{(i)}$ est une image du digit 2. On a dix chiffres donc 
$y^{(i)}$ est de dimension 10. 


\[ y = \begin{bmatrix}\label{matrice:y}
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    \end{bmatrix} \]

\clearpage

\noindent
\textbf{Implémentation}

    \begin{figure}[!h]
        \begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, fontsize=\footnotesize, linenos, breaklines=true]{python}
        y_matrix = np.zeros((num_labels,m)) 
    
        for i in range(m):
        for j in range(num_labels):
            y_matrix[j, i] = (j == y[i]-1).astype(int)
            
        # Compute Cost
        a1 = np.hstack((np.ones((X.shape[0], 1)), X))

        # Hidden Layer
        z2 = a1 @ theta1.T
        a2 = sigmoid(z2)
    
        # Add column 1's 
        a2 = np.hstack((np.ones((a2.shape[0], 1)), a2))

        # Output Layer
        z3 = a2 @ theta2.T

        a3 = sigmoid(z3) 

        J = (1/m) * np.sum(-y_matrix.T * np.log(a3) - (1 - y_matrix.T) * np.log(1-a3))
    
        """return 
        Cost at parameters (loaded from ex3weights): 0.287629 
        (this value should be about 0.287629)       
        """
        \end{minted}   
        \captionof{listing}{\label{lst:NonRegularizedCostFunction}Non Regularized Cost Function}
        \end{figure}

Ici, nous avons donc créé la nouvelle matrice \textit{y\_matrix} à partir de l'ensemble d'étiquettes de base, 
la matrice \textit{y}. Dans ce code, nous parcourons à l'aide de deux boucles \textit{m}, le nombre total 
d'échantillons d'entraînement, et \textit{num\_labels}, le nombre total de classes. Cela nous permet ensuite
de comparer la \(i\)-ème classe de \textit{y} avec la classe \(j\) courante. En utilisant la commande Python
\textit{.astype(int)}, nous convertissons le résultat \textit{True/False} en \textit{1/0}. \\
Au final, ce code parcourt chaque échantillon et chaque classe, puis remplit la matrice \textit{y\_matrix} de telle sorte que la classe
de chaque exemple soit indiquée par un \(1\) dans la ligne correspondante, et toutes les autres classes soient 
indiquées par des \(0\). \\

Le résultat du coût attendu est le bon, on peut considérer que notre fonction non régularisée est correcte. 

\clearpage

\subsection{Fonction de coût régularisée}

Dans cette section, on va régulariser la fonction de coût en s'appuyant 
sur la formule donnée pour un réseau de neurone. 

\begin{align}
    J(\Theta) &= \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left[ -y_k^{(i)} \log((h_\Theta(x^{(i)}))_{k}) - (1 - y_k^{(i)}) \log(1 - (h_\Theta(x^{(i)}))_{k}) \right] \nonumber \\
    &\quad + \frac{\lambda}{2m} \left[ \sum_{j=1}^{25} \sum_{k=1}^{400} (\Theta^{(1)}_{j,k}(1))^2 + \sum_{j=1}^{10} \sum_{k=1}^{25} (\Theta^{(2)}_{j,k}(2))^2 \right]
\end{align}

\noindent
\textbf{Implémentation}

\begin{figure}[!h]
    \begin{minted}[frame=lines, framesep=2mm, baselinestretch=1.2, fontsize=\footnotesize, linenos, breaklines=true]{python}

    # Cost regularisation
    reg = (Lambda / (2 * m)) * (np.sum(theta1[: , 1:] ** 2) + np.sum(theta2[:, 1:] ** 2))
    J = J + reg

    """return 
    Checking Cost Function (w/ Regularization) ...
    -------------------------- 
    Cost at parameters (loaded from ex3weights): 0.383770 
    (this value should be about 0.383770)      
    """
    \end{minted}   
    \captionof{listing}{\label{lst:NonRegularizedCostFunction}Regularized Cost Function}
\end{figure}


    Le résultat obtenu est correcte, notre fonction de coût est bel et bien régularisée. 

    




    





